{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc02b7c3-f751-4aea-857d-bf621317b249",
   "metadata": {},
   "source": [
    "# The Dataset\n",
    "\n",
    "The dataset is based on a similar dataset that was [published together with other datasets from fundamental physics](https://doi.org/10.1007/s41781-022-00082-6).\n",
    "\n",
    "Here we want to use a slighty less processed version to learn a bit about the physics content and how we can transform it such that it can be input to a neural network.\n",
    "\n",
    "It is provided in [Parquet](https://parquet.apache.org) format - written out using the [Awkward Array](https://awkward-array.org) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e202272-1714-43ef-bb1d-fceaa91cc34f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install \"awkward>=2\"\n",
    "# if not installed yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6857dc5-73e6-413f-8151-387f8fe03aaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import awkward as ak\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b70205-7d88-4340-8f52-26c9d810fe48",
   "metadata": {},
   "source": [
    "The dataset contains 4 \"row groups\" - each of them containing 100k events. Here we load only the first row group (index 0):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d465b52d-2cb3-4702-ad4f-afd0f85f7363",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d775ce-68f0-420c-a422-abc54ecdb763",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filename = \"smartbkg_dataset_4k.parquet\"\n",
    "url = \"https://zenodo.org/records/15303496/files/smartbkg_dataset_4k_training.parquet?download=1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c77ab9-f836-48f7-98b2-3908f6d427d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(filename):\n",
    "    urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0227e0c-3d29-4cce-9982-0369dd4f99ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ak_data = ak.from_parquet(filename, row_groups=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8e6051-9e6d-4205-bb77-175d3c9d7a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "ak_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76072b01-eec1-4c40-82e6-f2e13c3fd230",
   "metadata": {},
   "source": [
    "The structure of the data is the following\n",
    "- `particles`: generator-level quantities for each event - a variable length list of particles to be used as input for the training\n",
    "    - `pdg`: a number identifying the particle type according to the [numbering scheme of the Particle Data Group (PDG)](https://pdg.lbl.gov/2022/reviews/rpp2022-rev-monte-carlo-numbering.pdf)\n",
    "    - `index`: an identifier of the particle in a particular event\n",
    "    - `mother_index`: the `index` of the mother particle - this defines the decay tree\n",
    "    - the rest are generator-level features of the particles (all in the lab frame)\n",
    "        - `prodTime`: time of production (e.g. decay of mother particle) in ns\n",
    "        - `x, y, z`: positions of the production vertex in cm\n",
    "        - `px, py, pz, energy`: 4-momentum vector components in GeV (natural units with c=1)\n",
    "- `label`: `1` for events that **pass** the downstream event selection and `0` for those that **fail** it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dce2ad-8d04-4824-bd08-e544b60ab969",
   "metadata": {},
   "source": [
    "Here we will use a pandas DataFrame representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f788c8-b671-4d6d-9959-99c0bb750d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7204e8d7-0d3e-4ac8-b934-8cdd92cd4e2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_particles = ak.to_dataframe(ak_data.particles, levelname=lambda i: {0: \"event\", 1: \"particle\"}[i])\n",
    "df_particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0de8132-8177-4fc1-bcca-079cca2e8e58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = ak_data.label.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3779c1d-ebe5-45fc-9ebe-4b7129131e68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_label = pd.DataFrame(labels, columns=[\"label\"])\n",
    "df_label.index = df_label.index.rename(\"event\")\n",
    "df_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e1a67c-a6b5-41dc-afc6-22bdfdfe34ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_particles.join(df_label)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb531ab-1243-4aeb-bbf0-a0c8251f7248",
   "metadata": {},
   "source": [
    "First, let's plot the global distribution (across all events) for our particle features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb548a8-6c93-4980-af32-da4d8f9175ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_columns = [\"prodTime\", \"x\", \"y\", \"z\", \"energy\", \"px\", \"py\", \"pz\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bed14d-f809-4a3e-8ac9-602d9a3e92ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793674c5-07d7-45dc-be19-62ca9d821b45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_feats(df, column_names, bins=100, log=True, range_fn=None):\n",
    "    fig, axs = plt.subplots(ncols=4, nrows=2, figsize=(15, 5))\n",
    "    for ax, field in zip(axs.ravel(), column_names):\n",
    "        array = df[field].to_numpy()\n",
    "        if range_fn is None:\n",
    "            bin_range = None\n",
    "        else:\n",
    "            bin_range = range_fn(array)\n",
    "        ax.hist(array, bins=bins, range=bin_range)\n",
    "        if log:\n",
    "            ax.set_yscale(\"log\")\n",
    "        ax.set_title(field)\n",
    "    fig.tight_layout()\n",
    "    plt.close(fig)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86ba94d-b397-47e1-bc85-53796febb09d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_feats(df, feature_columns, log=True) # log scale, full range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e733b356-3b04-425f-9ecb-4034eda09a0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_feats(df, feature_columns, log=False, range_fn=lambda x: np.quantile(x, [0.05, 0.95])) # linear scale, 5% - 95% quantiles of distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992f1e3f-40d8-4e70-9eda-2a80046e0746",
   "metadata": {},
   "source": [
    "We can see a few characteristics of the experiment that have been taken into account in the simulation:\n",
    "- A peak in the energy at the total energy of colliding beams (7 GeV + 4 GeV = 11 GeV) - this corresponds to the Y(4S) resonance particles.\n",
    "- The next-lower peak in the energy is at around half of that - corresponding to the B mesons. The rest are then all the other particles in the decay chains.\n",
    "- The `z` vertex positions and momenta have a bias towards positive values (and a peak at 7 GeV - 4 GeV = 3 GeV for `pz` for the Y(4S) particles) - due to the asymmetric beam energies\n",
    "- The peak in `px` at around 0.46 GeV comes from the crossing angle of the beams - resulting in a small boost in x-direction for the Y(4S) particles.\n",
    "\n",
    "Note: particles with `x`, `y`, `z` values outside of the `[-10, 10]` range are removed for technical reasons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4df7484-37ec-4e04-bfc5-d48908ffe0df",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Exercise:</b> Overlay the distribution of features for events with label 0 and 1. Does the global distribution of features alone already provide discriminative power?\n",
    "<br><br>\n",
    "\n",
    "Hint: use e.g. `array[df.label==0]` to get the global distribution for label 0 events. To overlay both histograms you can use `histtype=\"step\"` or `alpha=0.5` as arguments to `plt.hist`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5de7f65-edbd-4124-81a0-c5938108a2cf",
   "metadata": {},
   "source": [
    "We can build decay trees using the `index` and `mother_index` fields. For visualization we use `grahviz` and to convert the PDG ids we use a dictionary to convert into a human readable unicode string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0933b783-645e-4ab4-b4a4-33594edda9f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pdg_to_unicode import pdg_to_unicode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fcf2c7-dabf-4eaa-9358-d14e6caba904",
   "metadata": {},
   "source": [
    "The first 3 particles of each event are typically the Y(4S) resonance `300553` decaying into a neutral B and anti-B meson:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e599c7-8325-467c-8648-aed264fc74e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.pdg.loc[0][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a411801-7414-45f5-9de8-49c1cbff4e8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdg_to_unicode[300553], pdg_to_unicode[-511], pdg_to_unicode[511]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d16070-ac3d-4ebf-ac01-11132315e829",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d865e3-9e40-4e8c-be59-e2bd1d93c701",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def draw_graph(x):\n",
    "    g = graphviz.Digraph()\n",
    "    for i, pdg in zip(x[\"index\"], x[\"pdg\"]):\n",
    "        g.node(str(i), label=pdg_to_unicode[pdg])\n",
    "    for src, dst in zip(x[\"mother_index\"], x[\"index\"]):\n",
    "        if src == dst: # particles without mother point to themselves\n",
    "            continue\n",
    "        g.edge(str(src), str(dst))\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bc419b-af47-48c2-971e-e4b2403f8480",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "event_id = 0 # change this to look at different events\n",
    "draw_graph(df.loc[event_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b121bdf5-51b9-49eb-aa81-7d362d324d1e",
   "metadata": {},
   "source": [
    "So each one of these decay trees (together with the features for each particle) will make up **one instance** of our training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95de451-c8b1-468c-b9f7-172da35cd96a",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "To use the data as input for a neural network we need to do some preprocessing.\n",
    "\n",
    "### Index the PDG ids\n",
    "First, we need to find a way to input the pdg ids.\n",
    "\n",
    "The numerical values are not very useful for processing in neural network layers, so we want to convert the particle identifier into a vector (compare e.g. word embeddings in a language model) - either with [one-hot encoding](https://en.wikipedia.org/wiki/One-hot#Machine_learning_and_statistics) or by utilizing an [Embedding layer](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html). In any case, we need to index (enumerate) them such that they are numbers in the range of `[0, num_pdg_ids]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e6b20f-a02d-4654-a646-c842f019b4ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unique_pdg_ids = np.unique(df.pdg.to_numpy())\n",
    "unique_pdg_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd917134-24cc-473c-a403-879f845421ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_pdg_ids = len(unique_pdg_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40d7ef3-9262-4d26-a7c5-883be53ef6ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_pdg_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7830966f-25d0-4e5e-8519-07632cc2c613",
   "metadata": {},
   "source": [
    "We will use the following dictionary to map the pdg ids. We will start counting at 1 since 0 will be a special padding value (more later)\n",
    "\n",
    "The value `num_pdg_ids + 1` will be reserved as a fallback token in case we encounter unseen pdg ids in the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f6a908-3e80-4b85-a477-a1756af8f387",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mapping = dict(zip(unique_pdg_ids.tolist(), range(1, num_pdg_ids + 1)))\n",
    "\n",
    "for (key, value), _ in zip(mapping.items(), range(10)):\n",
    "    print(f\"{key}: {value}\")\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d905a54-0db5-496c-86e4-d5a346532a84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def map_np(array, mapping, fallback=None):\n",
    "    \"\"\"\n",
    "    Apply a mapping over a numpy array - along the lines of\n",
    "    https://stackoverflow.com/a/16993364\n",
    "    \"\"\"\n",
    "    if fallback is None:\n",
    "        fallback = max(mapping.values()) + 1\n",
    "    # inv is the original array with the values replaced by their indices in the unique array\n",
    "    unique, inv = np.unique(array, return_inverse=True)\n",
    "    np_mapping = np.array([mapping.get(x, fallback) for x in unique])\n",
    "    return np_mapping[inv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edc271d-a640-4016-891e-282b34fb1a45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example:\n",
    "map_np(np.array([42, 753, 42, 1111, 753, 86277, 27786]), {42: 1, 753: 2, 1111: 3}, fallback=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77da91ca-ec41-4996-9543-85cef704ce8c",
   "metadata": {},
   "source": [
    "To have a consistent mapping for all datasets we will use the mapping defined in `pdg_mapping.json` which was produced by the script `create_pdg_mapping.py` that ran on all 4 row groups of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b35048-bf4f-4c07-a1fd-d12c342e4b82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96e6818-1694-4397-9b74-3e031ac12438",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"pdg_mapping.json\") as f:\n",
    "    pdg_mapping = dict(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fd4093-b176-4894-8834-65490cb89acc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# adds another array with the mapped particle ids to the DataFrame\n",
    "df[\"pdg_mapped\"] = map_np(df.pdg, pdg_mapping, fallback=len(pdg_mapping) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada253f8-7ddc-4d35-b23c-923f5254da1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.pdg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519e17ca-954c-48df-9486-26e531322170",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.pdg_mapped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e939dc3-ad35-4a75-a493-691d12b2d2eb",
   "metadata": {},
   "source": [
    "### List of arrays representation\n",
    "\n",
    "For loading the data into ML models it is useful to also have a representation of the data as a list of numpy arrays. We will have one list for each `pdg_mapped`, `index` and `mother_index` as well as one list of 2D numpy arrays for the particle features.\n",
    "\n",
    "To create this we will proceed as follows:\n",
    "* create \"flat\" numpy arrays (single array across event boundaries)\n",
    "* use a `pd.DataFrame.groupby` operation to get the indices of groups of particles\n",
    "* use these indices to create numpy arrays for each event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf14bc96-4c04-47a5-9d05-e7b039110058",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flat = {\n",
    "    \"features\": df[feature_columns].to_numpy(),\n",
    "    \"pdg_mapped\": df[\"pdg_mapped\"].to_numpy(),\n",
    "    \"index\": df[\"index\"].to_numpy(),\n",
    "    \"mother\": df[\"mother_index\"].to_numpy(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85abce45-14d2-41ce-bb46-bf8d4b3da001",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gb_indices = df.groupby(\"event\").indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773dd9e0-9117-4f15-a1ee-87d4043049c2",
   "metadata": {},
   "source": [
    "e.g. the following will give the indices of the first event:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afff94d5-1ca9-4fcc-9475-f55666740a2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gb_indices[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91976700-ff2b-49aa-b9fb-34e89ccad07b",
   "metadata": {},
   "source": [
    "Now we can fill the list of arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e71c3b-db74-485b-a985-03c503e329e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = {}\n",
    "for idx in gb_indices.values():\n",
    "    for k, array in flat.items():\n",
    "        data.setdefault(k, [])\n",
    "        data[k].append(array[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb949f8-a06f-492f-854d-1aaf00e25c58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data[\"features\"][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff08e79-6bf8-43d2-bda3-ada5815a9447",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data[\"pdg_mapped\"][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a53f0b-818b-4b2b-b013-ae4313ed1e2a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# A simple \"Deep Set\" Model\n",
    "\n",
    "As a starting point we will view our data as an **unordered set** of particles. The *Deep Set* model we will use applies a per **per-item transformation** ($\\phi$) followed by a **permutation invariant aggregation**, typically taking the sum/mean or min/max whose output can then be transformed ($\\rho$) by any means, e.g. another MLP.\n",
    "\n",
    "![](figures/deep_set_transformation.png)\n",
    "\n",
    "See [arXiv:1703.06114](https://arxiv.org/abs/1703.06114) for a detailed discussion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab02e3d5-6509-41ca-b676-7fc25f6645ee",
   "metadata": {},
   "source": [
    "The per-item transformation we can do easily do in `torch` by using a `Linear` layer - when operating on a sequence/set it will be applied per item:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005242bb-3090-44a2-8174-d1aac5ef02d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7408bf-1690-455b-ad59-e51dfbcadb1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "per_item_layer = nn.Linear(in_features=3, out_features=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b13c11-4fc0-49d6-8158-f530713d1b65",
   "metadata": {},
   "source": [
    "Let's create some example inputs to see what happens to them when they are passed through the layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee87589-7301-4265-8af8-eb110bb7a8d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = torch.rand(2, 5, 3)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ae5e6a-0301-44ba-8a7b-8865932d8140",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "per_item_layer(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f64414d-68c4-4b00-a477-c8cb23519d03",
   "metadata": {
    "tags": []
   },
   "source": [
    "For aggregation we just take the mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4465e165-7791-4d11-9408-13758e0ebef0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs.mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780923a5-6101-4065-8272-a419b932097b",
   "metadata": {},
   "source": [
    "A possible model (operating only on particle features) would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3025d7-0f68-426a-9482-d397c98f1819",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DeepSet(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super().__init__()\n",
    "        self.per_item_mlp = nn.Sequential(\n",
    "            nn.Linear(in_features, 32),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.global_mlp = nn.Sequential(\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "        )\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        x = self.per_item_mlp(x)\n",
    "        x = x.mean(axis=-2)\n",
    "        x = self.global_mlp(x)\n",
    "        return x\n",
    "\n",
    "model = DeepSet(len(feature_columns))\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3793d63-7c7a-46c8-b48e-d839fdc09d8e",
   "metadata": {},
   "source": [
    "The model can take arbitrary sized batches with an arbitrary sized set of features:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5c4aa6-f49e-4a4e-9bbb-dfa131e0b1e5",
   "metadata": {},
   "source": [
    "So this maps a set of particle features into a single number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4af3d60-1ee2-4d10-a472-504cfaf46ac0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = torch.tensor(data[\"features\"][0][np.newaxis, :])\n",
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebfb562-7bdd-4800-a018-49a0ff83bda2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d30b11d-8b36-430d-b344-b832ecb215d0",
   "metadata": {},
   "source": [
    "But what if not all sets in a batch of events have the same size?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88319df-2288-4483-9229-0907d3a45c5a",
   "metadata": {},
   "source": [
    "## 0-Padding\n",
    "\n",
    "Many standard operations in NN frameworks like PyTorch work only on arrays with same length lists for each subentry (there are also implementations for sparse computations for neural networks, but we won't consider these here). So we need to have **sequences of the same length, within one batch of data**.\n",
    "\n",
    "To achieve this we will **fill the values with 0** for instances in a batch with a lower number of particles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a019e7ab-b241-4dbc-8282-81a7ab8000d6",
   "metadata": {},
   "source": [
    "We can pad the sequences with the following helper function (similar to [`torch.nn.utils.rnn.pad_sequence`](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_sequence.html), but using numpy). To do this, we first create a matrix of zeros with the appropriate shape and data type. Then we replace the leftmost zeros row by row with the existing sequences. Due to the way numpy arrays work, this is much more efficient than concatenating each sequence with arrays of zeros and stacking them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72042a6f-6c41-4088-9df8-490537f60599",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pad_sequences(sequences, maxlen=None):\n",
    "    if maxlen is None:\n",
    "        maxlen = max(len(array) for array in sequences)\n",
    "    if sequences[0].ndim == 2:\n",
    "        shape = (len(sequences), maxlen, sequences[0].shape[-1])\n",
    "    else:\n",
    "        shape = (len(sequences), maxlen)\n",
    "    batch = np.zeros(shape, dtype=sequences[0].dtype)\n",
    "    for i, array in enumerate(sequences):\n",
    "        batch[i, :len(array)] = array\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9c2a76-c333-4689-bb3d-9dc1c7f1fafd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pad_sequences(data[\"pdg_mapped\"][:5]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c68a98-52be-4658-8b60-691ad4be5f94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pad_sequences(data[\"pdg_mapped\"][:5], maxlen=100).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c665aa7-a14e-486c-aa98-b4b71d607f25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pad_sequences(data[\"pdg_mapped\"][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4080fa19-9a68-456f-b0dc-30568371a91b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pad_sequences(data[\"features\"][:5]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f75cbb-d9e8-47b2-97aa-8fc2ec7547e2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "pad_sequences(data[\"features\"][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef10a0a2-d72a-4d38-bd18-5a98b3d6cdb6",
   "metadata": {},
   "source": [
    "## Masking\n",
    "\n",
    "A typical convention to treat values that are supposed to be ignored is to propagate a mask array through. In pytorch the convention is usually that values that are supposed to be ignored have a `True` in the mask and those that are not supposed to be ignored a `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd60ea1d-d78d-4598-b127-25ca79fc2ffd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_batch = np.zeros((2, 5, 4))\n",
    "test_batch[0, :3] = np.random.rand(3, 4)\n",
    "test_batch[1, :4] = np.random.rand(4, 4)\n",
    "test_batch = torch.tensor(test_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac35c50-9de5-4d19-92bb-50d4f69465de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d8c056-2920-4a01-86f8-9d08099f478f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mask = (test_batch == 0).all(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871c9f42-152a-4901-a158-f50e23feffdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff03849d-5bed-4f0b-b37f-9915ef00b8ef",
   "metadata": {},
   "source": [
    "The mask can be inverted using `~`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2c3ed4-b375-4849-b694-264578e87341",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "~mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8c6a9f-d7eb-42c9-a5f1-c7453827552d",
   "metadata": {},
   "source": [
    "We can compute a masked average by setting the masked values to 0 and then taking first the sum, followed by dividing my the sum of the inverse mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a5e869-d303-41c1-822b-71c859d875b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def masked_average(batch, mask):\n",
    "    batch = batch.masked_fill(mask[..., np.newaxis], 0)\n",
    "    sizes = (~mask).sum(axis=1, keepdim=True)\n",
    "    return batch.sum(axis=1) / sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c1ff37-c4bc-44e0-b027-82823436df47",
   "metadata": {},
   "source": [
    "Breaking this down step by step:\n",
    "\n",
    "* `batch.masked_fill(mask[..., np.newaxis], 0)` fill masked values with 0s\n",
    "* `[..., np.newaxis]` adds another dimension to ensure the same number of dimensions as the batch\n",
    "* `~mask` invert the mask\n",
    "* `(~mask).sum(axis=1, keepdim=True)` summing over not-masked values produces the sizes for each batch element. The argument `keepdim=True` ensures to keep the same number of dimensions as the batch.\n",
    "* `batch.sum(axis=1) / sizes` produces the average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64148a70-a844-4f02-b24b-0c7c09d5461a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "masked_average(test_batch, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8dd193-d589-4f2a-95a9-64e0ddfaed5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_batch.sum(axis=1) / torch.tensor([[3], [4]]) # works in this case since masked values are already 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4242afdc-2067-4d16-b79f-8ecebfc059ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_batch.mean(axis=1) # not the same, since it averages over all 5 elements!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505ccfea-a85b-403e-b2c2-cc754c1c2610",
   "metadata": {},
   "source": [
    "## Fit the model\n",
    "\n",
    "Putting everything together we can build a model as follows:\n",
    "- mask 0-padded entries\n",
    "- apply a the per-item transformation as a single `Linear` layer\n",
    "- calculate the mean across the sequence of particles\n",
    "- apply a number of global `Linear` layers on the averaged features\n",
    "- output a single number representing the probability of our `y` labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab0f464-0108-4c8a-a16e-277fb22ab588",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DeepSet(nn.Module):\n",
    "    def __init__(self, num_features=8, units=32):\n",
    "        super().__init__()\n",
    "        self.per_item_mlp = nn.Sequential(\n",
    "            nn.Linear(num_features, units),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.global_mlp = nn.Sequential(\n",
    "            nn.Linear(units, units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(units, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, inputs, mask=None):\n",
    "        x = inputs\n",
    "        x = self.per_item_mlp(x)\n",
    "        if mask is not None:\n",
    "            x = masked_average(x, mask)\n",
    "        else:\n",
    "            x = x.mean(axis=-2)\n",
    "        x = self.global_mlp(x)\n",
    "        return x\n",
    "    \n",
    "model = DeepSet()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea7b5f8-dc4d-425b-b0b4-5996495c9954",
   "metadata": {},
   "source": [
    "We now fit the model using a random 10% fraction of the dataset for validation during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace657cd-9731-45df-bd9e-6462a8e81fd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13368b0a-4b1b-4df6-8e3f-1ad464df5f6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(data[\"features\"], labels, test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d7b394-8d0a-419e-b7d6-174fe27e92c5",
   "metadata": {},
   "source": [
    "We will create a `Dataset` that inherits from `torch.utils.data.Dataset` that will provide `x` (input) and `y` (target) pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4834150b-fc95-462e-a72d-4ee5488647be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadd0b9b-ecb9-4677-b167-c90163e928b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        super().__init__()\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        return self.x[i], self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da82b5be-3a2a-4adc-9f39-4410b65fcb43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_train = Dataset(x_train, y_train)\n",
    "ds_val = Dataset(x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52933a71-2a7e-4ffe-ac42-c6009a4d19fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x, y = ds_train[0]\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62e6711-9521-454b-b237-e83474d55dce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5cb0e2-c2ee-48d0-9e3c-a046511aea03",
   "metadata": {},
   "source": [
    "We will then use a `DataLoader` to put these instances into batches. We have to provide a function that applies the `pad_sequences` to our input features and calculates the mask then as the `collate_fn` argument to our `DataLoader`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fec02b-8ee7-4448-a29c-cea89cbcdb9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate_fn(inputs):\n",
    "    x = [i[0] for i in inputs]\n",
    "    y = [i[1] for i in inputs]\n",
    "    x = torch.tensor(pad_sequences(x))\n",
    "    y = torch.tensor(y)\n",
    "    mask = (x == 0).all(axis=-1)\n",
    "    return x, y, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e7d922-c2d9-4755-a314-be0dec0b6a65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for batch in DataLoader(ds_train, batch_size=256, collate_fn=collate_fn, shuffle=True):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c318c0-92ad-4942-8885-2893e27801e0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c6b39a-d052-4da8-923f-1e9cd5448ab1",
   "metadata": {},
   "source": [
    "Now we can implement the training loop:\n",
    "\n",
    "For the loss we will use the binary cross entropy - `with_logits` means we use outputs without a sigmoid activation function applied.\n",
    "\n",
    "The activation function is applied in the loss function instead, allowing a more numerically stable computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891f84e9-9b03-4e7f-a07e-6537f3f60bc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss_fn(logits, y):\n",
    "    return F.binary_cross_entropy_with_logits(logits.squeeze(), y.float())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8263c460-18c3-483a-bb11-2c24424a8421",
   "metadata": {},
   "source": [
    "We also want to track the accuracy - the fraction of correctly labelled events taking the most likely label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4764cf-7420-4a1d-ad4f-0e7b7846f264",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def accuracy_fn(logits, y):\n",
    "    return (logits.squeeze().sigmoid().round() == y).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb0e0af-07b6-4acc-abde-ebaf96df9da2",
   "metadata": {},
   "source": [
    "Now we implement the training loop. We use the adam optimizer with default parameters for now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad4ba9c-70e5-4605-9bbd-f4b2fbdf1db6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fit(model, dl_train, dl_val, epochs=10, device=\"cpu\", history=None):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    def train_step(x, y, mask):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x, mask=mask)\n",
    "        loss = loss_fn(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return logits.detach().cpu(), loss.detach().cpu()\n",
    "\n",
    "    def test_step(x, y, mask):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = model(x, mask=mask)\n",
    "            return logits.cpu(), loss_fn(logits, y.to(device)).cpu()\n",
    "\n",
    "    def to_device(x, y, mask):\n",
    "        if isinstance(x, dict):\n",
    "            x = {k: v.to(device) for k, v in x.items()}\n",
    "        else:\n",
    "            x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        mask = mask.to(device)\n",
    "        return x, y , mask\n",
    "\n",
    "    if history is None:\n",
    "        history = []\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch}\")\n",
    "        train_loss = []\n",
    "        train_acc = []\n",
    "        val_loss = []\n",
    "        val_acc = []\n",
    "        for i, (x, y, mask) in enumerate(dl_train):\n",
    "            x, y, mask = to_device(x, y, mask)\n",
    "            logits, loss = train_step(x, y, mask)\n",
    "            train_loss.append(float(loss))\n",
    "            train_acc.append(float(accuracy_fn(logits, y)))\n",
    "            print(\n",
    "                f\"Batch {i:03d}/{len(dl_train)}, \"\n",
    "                f\"Train loss: {np.mean(train_loss):.3f}, \"\n",
    "                f\"Train accuracy: {np.mean(train_acc):.3f}\",\n",
    "                end=\"\\r\" if i != len(dl_train) - 1 else \", \",\n",
    "                flush=True,\n",
    "            )\n",
    "        for x, y, mask in dl_val:\n",
    "            x, y, mask = to_device(x, y, mask)\n",
    "            logits, loss = test_step(x, y, mask)\n",
    "            val_loss.append(float(loss))\n",
    "            val_acc.append(float(accuracy_fn(logits, y)))\n",
    "        print(\n",
    "            f\"Validation loss: {np.mean(val_loss):.3f}, \"\n",
    "            f\"Validation accuracy: {np.mean(val_acc):.3f}\"\n",
    "        )\n",
    "        history.append(\n",
    "            {\n",
    "                \"loss\": np.mean(train_loss),\n",
    "                \"val_loss\": np.mean(val_loss),\n",
    "                \"acc\": np.mean(train_acc),\n",
    "                \"val_acc\": np.mean(val_acc),\n",
    "            }\n",
    "        )\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fddfcc7-3327-4889-8254-179676458bbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = []\n",
    "\n",
    "dl_opts = dict(batch_size=256, collate_fn=collate_fn)\n",
    "dl_train = DataLoader(ds_train, shuffle=True, **dl_opts)\n",
    "dl_val = DataLoader(ds_val, **dl_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcb6526-5464-4ce2-acca-2cda3d8c2d5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = fit(model, dl_train, dl_val, history=history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f98ddd-964c-49b2-95c8-f93cf5f037f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e2afc1-aaa2-46e0-9ed2-2d6374241942",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(history).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcf2e69-9adf-47e6-b635-2d42f53d4f38",
   "metadata": {},
   "source": [
    "# Embedding layers and multiple inputs\n",
    "\n",
    "So far we have not used the `pdg` field - the particle type information. One way to use such categorical features is to feed them through an [`Embedding`](https://keras.io/api/layers/core_layers/embedding/) layer.\n",
    "\n",
    "Since we have mapped the PDG ids to numbers in a continuous range we can directly use such a layer - remember that we shifted the numbers by 1 to be able to use 0 as a padding value. The number of output dimensions is a hyperparameter of this layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4916ee5b-db03-403c-a5b8-c65a7a05f08e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embed_dim = 8\n",
    "embedding = nn.Embedding(num_pdg_ids + 1, embed_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b820e222-d3f6-43b3-b79c-b12cb1f08519",
   "metadata": {
    "tags": []
   },
   "source": [
    "All this layer does is to have a learnable matrix of size `(num_categories, embed_dim)` that maps each category to a vector of fixed size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3df161-3687-46bc-b318-c327b1f90ee7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding(torch.tensor(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196feb8d-1c17-4373-aa4a-0c06f091fee2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding(torch.tensor(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c48787-0735-40f8-8621-015c425fa102",
   "metadata": {},
   "source": [
    "It essentially just picks the row with the specified index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055a6f14-e81e-48a7-b1b2-778f4fe67443",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd523db8-3de9-4800-a366-35231379fc86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding.weight[1] == embedding(torch.tensor(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088c1f8c-9b39-4edc-806d-af1eb1b9bd21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding.weight[2] == embedding(torch.tensor(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a391fd8d-5f70-4e9f-94cb-915fbbf8345c",
   "metadata": {},
   "source": [
    "This is equivalent to applying a Dense layer to one-hot encoded categorical features.\n",
    "\n",
    "To use both PDG ids and the rest of the particle features as inputs you can create a model that takes multiple inputs and then concatenate the embedded PDG ids with the other features, e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15845347-f70c-4869-94ba-915ddd583fc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, num_feat=8, embed_dim=8, num_pdg_ids=num_pdg_ids, units=32):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_pdg_ids + 1, embed_dim)\n",
    "        self.deep_set = DeepSet(num_features=num_feat + embed_dim, units=units)\n",
    "\n",
    "    def forward(self, inputs, mask=None):\n",
    "        pdg = inputs[\"pdg\"]\n",
    "        feat = inputs[\"feat\"]\n",
    "        emb = self.embedding(pdg)\n",
    "        x = torch.cat([feat, emb], -1)\n",
    "        return self.deep_set(x, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d015a9-a550-40a6-9df8-f48e40fecf38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "combined_model = CombinedModel()\n",
    "combined_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12314ce-9dd1-4913-aa10-89fe8c53b1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(data[\"features\"][0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8fae40-0c7b-44a5-ac99-f4acf8b27c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(data[\"features\"][0]).unsqueeze(0).shape # alternative to [np.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda84d75-318b-4115-a07b-30b56b71b259",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = dict(\n",
    "    pdg=torch.tensor(data[\"pdg_mapped\"][0]).unsqueeze(0),\n",
    "    feat=torch.tensor(data[\"features\"][0]).unsqueeze(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0f6565-cc4c-42d2-8535-4755c8aaa945",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "combined_model(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298fa963-678c-4ad6-abb9-26a171a13aeb",
   "metadata": {},
   "source": [
    "Note: When fitting such a model you need to adjust the `Dataset` and the `collate_fn` to provide the inputs as a dictionary with fields `\"pdg\"` and `\"feat\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13018da-3920-4d85-bf49-72b61d42df03",
   "metadata": {},
   "source": [
    "# Graph Network\n",
    "\n",
    "Another thing we haven't used yet is the graph structure of the events (the particles form a decay tree). One way to incorporate this is via Graph Convolutions.\n",
    "\n",
    "## Graph Convolutions\n",
    "\n",
    "Similar to convolutional networks where we update the state of each pixel by aggregating over neigboring pixels we can perform a *graph convolution* by aggregating over neighboring nodes in a graph:\n",
    "\n",
    "![cnn vs gcn](figures/cnn_vs_gcn.jpg)\n",
    "\n",
    "(figure from https://zhuanlan.zhihu.com/p/51990489)\n",
    "\n",
    "In the \"Deep sets\" language such a graph convolution corresponds to a *permutation equivariant* tranformation of the set of nodes, since it also does not depend on the ordering if the aggregation is done in a permutation invariant way (e.g. sum/mean/min/max).\n",
    "\n",
    "A rather simple implementation is given by the update rule introduced in [arXiv:1609.02907](https://arxiv.org/abs/1609.02907) (\"GCN\")\n",
    "\n",
    "$ H^{(l+1)} = \\sigma(\\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}H^{(l)}W^{(l)}) $\n",
    "\n",
    "where $A$ is the *adjacency matrix*, $D$ the *degree matrix*,  $H^{(l)}$ the hidden state of layer $l$ and $W^{(l)}$ the weight matrix of the layer $l$. The tilde above $A$ and $D$ indicates that self-loops were added (all nodes are neighbors of themselves).\n",
    "\n",
    "An equivalent formulation is\n",
    "\n",
    "$ h_i^{(l+1)} = \\sigma\\left(\\sum\\limits_{j\\in\\mathcal{N}(i)}\\frac{1}{c_{ij}}h^{(l)}_j W^{(l)}\\right) $\n",
    "\n",
    "where $ \\mathcal{N(i)} $ is the set of neighbors of node $i$ and $c_{ij} = \\sqrt{N_i}\\sqrt{N_j}$ with $N_i$ being the number of neigbors of node $i$\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Exercise:</b> Verify for one example event that the matrix multiplication of the adjacency matrix with the feature matrix is equivalent to taking the sum over neighbor features for each node. In other words that\n",
    "    \n",
    "$ (AF)_{ij} = \\sum\\limits_{k\\in\\mathcal{N}(i)}F_{kj} $\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8745038c-36a8-4fca-b05f-e9466dca2878",
   "metadata": {},
   "source": [
    "e.g with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8efd91-c890-461c-9eac-e2945b2f0974",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "F = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "F # 3 nodes, 2 features each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e715c10e-9ceb-4e85-808d-e69ac4a7352f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "A = np.array([[1, 1, 0], [1, 1, 1], [0, 1, 1]])\n",
    "A # all nodes are neighbors to themselves, node1 is neighbor of node2 and vice versa, node2 is neighbor of node3 and vice versa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e0cb65-daa4-4b49-93ef-5a891a970713",
   "metadata": {},
   "source": [
    "We can decompose the GCN into an operation that normalizes the adjacency matrix via the node degrees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522785c9-9761-4f28-a646-db6ed72fb70d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalize_adjacency(adj):\n",
    "    deg_diag = adj.sum(axis=2)\n",
    "    deg12_diag = torch.where(deg_diag != 0, deg_diag**-0.5, 0)\n",
    "    # normalization coefficients are outer product of inverse square root of degree vector\n",
    "    # gives coeffs_ij = 1 / sqrt(N_i) / sqrt(N_j)\n",
    "    coeffs = deg12_diag[:, :, np.newaxis] @ deg12_diag[:, np.newaxis, :]\n",
    "    return adj.float() * coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ab871e-5b97-4ae8-9b56-e74702346ae3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "normalize_adjacency(torch.tensor(A)[np.newaxis, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c538d2-7331-4f06-a74b-6725744e93de",
   "metadata": {},
   "source": [
    "and the update rule that takes the node inputs and the adjacency matrix as parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d28c37-2c92-4b1f-9603-23c0b2c33e68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple graph convolution. Equivalent to GCN from Kipf & Welling (https://arxiv.org/abs/1609.02907)\n",
    "    when fed a normalized adjacency matrix.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, units):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features, units)\n",
    "\n",
    "    def forward(self, inputs, adjacency):\n",
    "        return adjacency @ self.linear(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb646503-e39d-4c6b-82ab-f7ec18507fd8",
   "metadata": {},
   "source": [
    "## Adjacency matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86354204-5787-48d4-a736-a841a2adfac7",
   "metadata": {},
   "source": [
    "To get the adjacency matrices in our dataset we create need to create a matrix where the `index` is equal to the `mother` field, e.g. for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5070242-1d18-4482-98f8-2323a24c493a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "index = np.array([1, 2, 3])\n",
    "mother = np.array([1, 1, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909d0ce9-7751-4e75-b013-cfcc071615e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "we do "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b113de-8484-4371-81ca-7e995f5c7726",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "index[:, np.newaxis] == mother[np.newaxis, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5802a7e1-cd29-426e-b1df-ed9fc66f0662",
   "metadata": {},
   "source": [
    "we used slicing with `np.newaxis` to add extra dimensions to the array, so we made a comparison between a column vector and a row vector which numpy automatically broadcasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bc7899-ffc8-408a-be06-b3c09501df0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "index[:, np.newaxis], mother[np.newaxis, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45ed010-5380-45db-8fae-10da70f19695",
   "metadata": {},
   "source": [
    "We will now create adjacency matrices for all events:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04789581-8902-4ef6-95df-171ebcc334e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_adj(index, mother):\n",
    "    return (\n",
    "        (mother[np.newaxis, :] == index[:, np.newaxis]) # mother-daughter relations\n",
    "        | (index[np.newaxis, :] == mother[:, np.newaxis]) # daughter-mother relations\n",
    "        | (index[np.newaxis, :] == index[:, np.newaxis]) # self loops\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49052914-4a67-419e-a4f7-97f71d7d5e4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data[\"adj\"] = [get_adj(index, mother) for index, mother in zip(data[\"index\"], data[\"mother\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80801bb1-09de-4b5a-855c-1e8d8618b428",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.imshow(data[\"adj\"][0], cmap=\"Greys\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25121ad0-4585-48d8-9a48-bcf167f35a2d",
   "metadata": {},
   "source": [
    "This translates in the same graphs we have seen before, but now we have both mother-daughter and daughter-mother connections and also a connection from each node to itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bc2355-207d-4c2f-9bb5-f814782d4c04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def draw_graph_from_adjacency(adj, pdg):\n",
    "    g = graphviz.Digraph()\n",
    "    for i, pdg_i in enumerate(pdg):\n",
    "        g.node(str(i), label=pdg_to_unicode[pdg_i])\n",
    "    for src in range(len(pdg)):\n",
    "        for dst in range(len(pdg)):\n",
    "            if adj[src][dst]:\n",
    "                g.edge(str(src), str(dst))\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39569d69-4e0e-4b94-8d4a-ce37f4ce1634",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "event_index = 0 # change to look at different events\n",
    "draw_graph_from_adjacency(data[\"adj\"][event_index], df.loc[event_index].pdg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc2614a-a3bd-4cff-ba9f-770818fb8f35",
   "metadata": {
    "tags": []
   },
   "source": [
    "When iterating over the training dataset later to fit a model we also need to create batches of adjacency matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ee7f1c-8230-4bd0-836b-cd66b09045a9",
   "metadata": {},
   "source": [
    "Here we also need to pad the batches of adjacency matrices to the maximum event length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d1d652-4dbd-4b71-9c34-bfe9f0f8d5f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pad_adjacencies(adj_list):\n",
    "    maxlen = max(len(adj) for adj in adj_list)\n",
    "    batch = np.zeros((len(adj_list), maxlen, maxlen), dtype=bool)\n",
    "    for i, adj in enumerate(adj_list):\n",
    "        batch[i, :len(adj), :len(adj)] = adj\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6deadd-6ff1-435f-813b-ef73b00c7359",
   "metadata": {},
   "source": [
    "So each batch will have features, pdg ids and adjacency matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c8e08a-9a68-4023-8c0e-9650bdbbf385",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pad_sequences(data[\"pdg_mapped\"][:256]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b545a196-b8a8-4fba-84fb-cd769d87e550",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pad_sequences(data[\"features\"][:256]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2393bd-daa2-4559-82ad-135558ed8224",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pad_adjacencies(data[\"adj\"][:256]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d4000e-dceb-46c6-a7ec-68cdb47f76d5",
   "metadata": {},
   "source": [
    "Putting it together a `Dataset` and `collate_fn` for the `DataLoader` for this could look like the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b08651-ecbc-4b03-9f6e-d7730c88bbc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GraphDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, feat, pdg, adj, y):\n",
    "        self.feat = feat\n",
    "        self.pdg = pdg\n",
    "        self.adj = adj\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.feat)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        x = {\n",
    "            \"feat\": self.feat[i],\n",
    "            \"pdg\": self.pdg[i],\n",
    "            \"adj\": self.adj[i]\n",
    "        }\n",
    "        y = self.y[i]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e0567f-a18f-4f1e-94c6-bbb65b409521",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate_fn_graphs(inputs):\n",
    "    feat, pdg, adj = [\n",
    "        [x[key] for x, y in inputs] for key in [\"feat\", \"pdg\", \"adj\"]\n",
    "    ]\n",
    "    y = [y for x, y in inputs]\n",
    "    x = {\n",
    "        \"feat\": torch.tensor(pad_sequences(feat)),\n",
    "        \"pdg\": torch.tensor(pad_sequences(pdg)),\n",
    "        \"adj\": torch.tensor(pad_adjacencies(adj)),\n",
    "    }\n",
    "    y = torch.tensor(y)\n",
    "    mask = (x[\"feat\"] == 0).all(axis=-1)\n",
    "    return x, y, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3116f3c2-b955-45ce-b584-1cd173c34325",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dl = DataLoader(\n",
    "    GraphDataset(feat=data[\"features\"], pdg=data[\"pdg_mapped\"], adj=data[\"adj\"], y=labels),\n",
    "    batch_size=256,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn_graphs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f64031-96c8-4a46-8296-70628bd14570",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f03d81-bc35-4603-826f-744f8f9e4f59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for batch in tqdm(dl):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394866ea-386f-4a45-8703-24669e6c4f64",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "x, y, mask = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1b99ee-dd6e-4e5a-89b2-15dd1ae23830",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[\"feat\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9661b67-cf90-4008-a4fd-3257ca035c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[\"pdg\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465e40a2-2417-45a5-8192-72818b31b328",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[\"adj\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecdbf82-730b-469d-961c-01a79c75435d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b06f69a-83e9-49e6-8abc-ed81c7f6c887",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f64a35b-7fc5-449f-b718-dde1ac3de343",
   "metadata": {},
   "source": [
    "For using `GCN` layers in a model one needs the adjacency matrices as an additional input and feed them through `normalize_adjaceny` once before passing them over to a `GCN` layer. Here an example for a torch model that only applies a single `GCN` layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad67573-9e68-4621-b8d0-b3847619308c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GraphNetwork(nn.Module):\n",
    "    def __init__(self, in_features, units=32):\n",
    "        super().__init__()\n",
    "        self.gcn = GCN(in_features, units)\n",
    "\n",
    "    def forward(self, inputs, mask=None):\n",
    "        adj = inputs[\"adj\"]\n",
    "        feat = inputs[\"feat\"]\n",
    "        adj = normalize_adjacency(adj)\n",
    "        return self.gcn(feat, adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e492a475-0b2a-4f2c-9cbc-501042285703",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "graph_network = GraphNetwork(len(feature_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be77cde6-8b09-433d-b4eb-beb7c5cdfd4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out = graph_network(x)\n",
    "out.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

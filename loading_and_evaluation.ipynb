{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e35083b-f9ab-4d9d-ae9c-9e7227fef04d",
   "metadata": {},
   "source": [
    "# Loading and Evaluating Multiple Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c56162c-543f-423e-9075-b7f31feb317c",
   "metadata": {},
   "source": [
    "This notebooks serves as a guide on how to load multiple models that were saved in the way described in [workbook.ipynb](workbook.ipynb). We will also look at how to evaluate them using an independent test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f2e89a-a2be-4f7c-b168-1c0bf67cb2fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "import awkward as ak\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from models import from_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7259b5cc-1d76-4006-9dc4-7b6338a29a8e",
   "metadata": {},
   "source": [
    "# Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d1e569-ca01-4939-b202-0fbfa214db67",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = Path(\"saved_models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f6cfef-d816-4ada-a44e-076f2c5ddbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -la $save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d806cb4-b299-4af3-9cf0-fc2fa216f8b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# manually select models to evaluate\n",
    "configs = [\"deepset\"]\n",
    "\n",
    "# alternative: all models in `save_path`\n",
    "#configs = [path.name for path in save_path.iterdir()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7880c5b7-7e4e-4e6b-bd16-b03a8f330b4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "models = {}\n",
    "for tag in configs:\n",
    "    model_path = save_path / tag\n",
    "    with open(model_path / \"config.json\") as f:\n",
    "        config = json.load(f)\n",
    "    model = from_config(config)\n",
    "    state = torch.load(model_path / \"state.pt\")\n",
    "    model.load_state_dict(state)\n",
    "    models[tag] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a70baf7-7a54-4deb-9c8f-e21d3b414d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53444cb5-9371-455f-8a24-bcc785ab1093",
   "metadata": {},
   "source": [
    "# Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a93ad2d-15d9-4cc5-ac9f-c5bdb1d6b28c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import GraphDataset, load_data, map_np, collate_fn, preprocess, get_adj, loss_fn, accuracy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d775ce-68f0-420c-a422-abc54ecdb763",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filename = \"smartbkg_dataset_4k_testing.parquet\"\n",
    "url = ... # url for testdata will be provided on second lab day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c77ab9-f836-48f7-98b2-3908f6d427d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(filename):\n",
    "    urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d06ded-616e-4746-81f8-f9604434f01f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_columns = [\"prodTime\", \"x\", \"y\", \"z\", \"energy\", \"px\", \"py\", \"pz\"]\n",
    "df, labels = load_data(filename, row_groups=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d479259d-df68-4faf-b4a9-313adde00829",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"pdg_mapping.json\") as f:\n",
    "    pdg_mapping = dict(json.load(f))\n",
    "df[\"pdg_mapped\"] = map_np(df.pdg, pdg_mapping, fallback=len(pdg_mapping) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d014b5-b98b-4dfd-adf1-adebf06a0921",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = preprocess(df, pdg_mapping=pdg_mapping, feature_columns=feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3025b28c-1d8a-43be-b3b0-016e5bb9d6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"adj\"] = [get_adj(index, mother) for index, mother in zip(data[\"index\"], data[\"mother\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1803459-e4bd-4d0a-8fcb-aeed77205f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = torch.utils.data.DataLoader(\n",
    "    GraphDataset(feat=data[\"features\"], pdg=data[\"pdg_mapped\"], adj=data[\"adj\"], y=labels),\n",
    "    batch_size=256,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9baab038-f39b-4eae-9c92-f9da0b805ec3",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dc3046-2b11-4247-bb67-528439b7590c",
   "metadata": {},
   "source": [
    "If we let some test data run through one of our trained models, each event in that dataset will be mapped to a number between 0 and 1. This is due to the sigmoid activation function used for every last layer. The output value can be interpreted as the confidence the models has in a particular event passing the skimming. In order to decide which events get thrown away prior to the detector simulation, a threshold needs to be selected. That way every event which generated an output less than the threshold gets thrown away and the others are kept. To each threshold, there is a corresponding pair of true and false positive rates (FPR, TPR). We can plot them against each other, leading to a *reciever operating characteristic* (ROC) curve. Looking at the graphic below, this curve can be used to evaluate the model's performance, which can be quantified by calculating the area under the ROC curve (auc).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebd826f-22ff-4753-ad1f-5c05a2871ebe",
   "metadata": {},
   "source": [
    "![](figures/Roc_curve.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c2a6ae-99d6-486a-89fe-747215291016",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, accuracy_score, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490b0152-c07a-402e-ac32-6423303ee6db",
   "metadata": {},
   "source": [
    "The following function will run our model once through the DataLoader to produce pairs of model outputs (`logits`) and targets (`y`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5417154-b95f-49d5-b002-a94f9c79a41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dl):\n",
    "    model.eval()\n",
    "    out_y = []\n",
    "    out_logits = []\n",
    "    for x, y, mask in tqdm(dl):\n",
    "        out_y.append(y)\n",
    "        with torch.no_grad():\n",
    "            logits = model(x, mask=mask)\n",
    "        out_logits.append(logits.squeeze())\n",
    "    return torch.cat(out_y), torch.cat(out_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21327533-9494-436a-aa05-220f7007a102",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {}\n",
    "for name, model in models.items():\n",
    "    print(\"Evaluating\", name)\n",
    "    scores[name] = evaluate(model, dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f720143f-d842-4fb5-9f87-2d517ffbe169",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for name, (y_true, y_logits) in scores.items():\n",
    "    y_score = y_logits.sigmoid()\n",
    "    fpr, tpr, thr = roc_curve(y_true.numpy(), y_score.numpy())\n",
    "    plt.plot(fpr, tpr, label=name)\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50d8486-6c6e-4a92-9216-6cd298a1aa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = []\n",
    "for (name, model), (y_true, y_logits) in zip(models.items(), scores.values()):\n",
    "    y_score = y_logits.sigmoid()\n",
    "    fpr, tpr, thr = roc_curve(y_true.numpy(), y_score.numpy())\n",
    "    loss = loss_fn(y_logits, y_true)\n",
    "    accuracy = accuracy_fn(y_logits, y_true)\n",
    "    summary.append(\n",
    "        {\n",
    "            \"Model\": name,\n",
    "            \"Loss\": loss.item(),\n",
    "            \"Accuracy\": accuracy.item(),\n",
    "            \"AUC\": auc(fpr, tpr),\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cd0e0f-9f47-49a1-937a-f4a03472d51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9592590-7a4d-49f5-a61a-376f16c3b69b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(summary).set_index(\"Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a43f383-7960-4f0f-8d00-1b508b9718e5",
   "metadata": {},
   "source": [
    "# Speedup estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339ac9e0-4287-43db-ad07-8269ddbbe4e8",
   "metadata": {},
   "source": [
    "How do we finally choose a threshold? We want our model to deliver the highest possible speedup to our simulation chain. With certain assumptions (see [labday.md](labday.md)), a formula can be derived that only depends on FPR, TPR. This will be your job. With that you can plot against the thresholds a speedup curve and find its maximum as well as the corresponding optimal threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086ad801-5048-4580-a166-383dbd339df6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def speedup(fpr, tpr):\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9b687a-4b68-41a4-b79d-58b6656aa10c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, (model_name, score) in enumerate(scores.items()):\n",
    "    fpr, tpr, thr = roc_curve(labels, score)\n",
    "    s = speedup(fpr, tpr)\n",
    "    s_max = max(s)\n",
    "    thr_max = thr[np.argmax(s)]\n",
    "    summary[i][\"Max. Speedup\"] = s_max\n",
    "    summary[i][\"Best Threshold\"] = thr_max\n",
    "    plt.plot(thr[1:], s[1:], label=model_name)\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Speedup\")\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc3ed86-1e3c-4175-af0d-c75d7fb3a63a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(summary).set_index(\"Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c2e4cf-882e-4702-98bb-e2a6444da875",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

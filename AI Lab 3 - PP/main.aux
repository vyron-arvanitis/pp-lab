\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{ai-lab-repo}
\citation{ai-lab-repo}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Smart simulation strategy: a neural network filters generated events before expensive detector simulation. Figure adapted from the official LMU AI Lab repository\nobreakspace  {}\cite  {ai-lab-repo}.}}{2}{figure.1}\protected@file@percent }
\newlabel{fig:smart_sim}{{1}{2}{Smart simulation strategy: a neural network filters generated events before expensive detector simulation. Figure adapted from the official LMU AI Lab repository~\cite {ai-lab-repo}}{figure.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}The Belle experiment}{2}{section.2}\protected@file@percent }
\citation{belle2-image}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Closeup of the Belle\nobreakspace  {}II detector indicating all the different subdetectors\nobreakspace  {}\cite  {belle2-image}.}}{3}{figure.2}\protected@file@percent }
\newlabel{fig:belle-detector-overview}{{2}{3}{Closeup of the Belle~II detector indicating all the different subdetectors~\cite {belle2-image}}{figure.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Belle\nobreakspace  {}II subdetectors and the particles they primarily detect.}}{3}{table.1}\protected@file@percent }
\newlabel{table:belle-subdetectors}{{1}{3}{Belle~II subdetectors and the particles they primarily detect}{table.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Problem Description}{3}{subsection.2.1}\protected@file@percent }
\citation{belle2-Y-resonances}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Cross section of $e^+e^-$ with respect to the center-of-mass energy [GeV] \cite  {belle2-Y-resonances}.}}{4}{figure.3}\protected@file@percent }
\newlabel{fig:upsilon-resonances}{{3}{4}{Cross section of $e^+e^-$ with respect to the center-of-mass energy [GeV] \cite {belle2-Y-resonances}}{figure.3}{}}
\citation{zaheer2017deep}
\@writefile{toc}{\contentsline {section}{\numberline {3}Theory}{5}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Deep Sets}{5}{subsection.3.1}\protected@file@percent }
\newlabel{sec:DeepSet}{{3.1}{5}{Deep Sets}{subsection.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Schematic illustration of a Deep Set model.}}{5}{figure.4}\protected@file@percent }
\newlabel{fig:deepset}{{4}{5}{Schematic illustration of a Deep Set model}{figure.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Graph Convolutional Neural Networks}{5}{subsection.3.2}\protected@file@percent }
\newlabel{sec:GCN}{{3.2}{5}{Graph Convolutional Neural Networks}{subsection.3.2}{}}
\citation{zhihu-figure-qc}
\citation{zhihu-figure-qc}
\citation{turing}
\citation{DBLP:journals/corr/VaswaniSPUJGKP17}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Analogy between CNNs and GCNs. In CNNs, pixels are updated based on spatial neighborhoods; in GCNs, nodes are updated based on graph neighborhoods \cite  {zhihu-figure-qc}.}}{6}{figure.5}\protected@file@percent }
\newlabel{fig:gcn}{{5}{6}{Analogy between CNNs and GCNs. In CNNs, pixels are updated based on spatial neighborhoods; in GCNs, nodes are updated based on graph neighborhoods \cite {zhihu-figure-qc}}{figure.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Multi Layer Perceptron}{6}{subsection.3.3}\protected@file@percent }
\newlabel{sec:theory_mlp}{{3.3}{6}{Multi Layer Perceptron}{subsection.3.3}{}}
\citation{DBLP:journals/corr/VaswaniSPUJGKP17}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Schematic illustration of a Multi‐Layer Perceptron \cite  {turing}.}}{7}{figure.6}\protected@file@percent }
\newlabel{fig:mlp}{{6}{7}{Schematic illustration of a Multi‐Layer Perceptron \cite {turing}}{figure.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Transformer}{7}{subsection.3.4}\protected@file@percent }
\newlabel{sec:Transformer}{{3.4}{7}{Transformer}{subsection.3.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Transformer-model architecture \cite  {DBLP:journals/corr/VaswaniSPUJGKP17}}}{8}{figure.7}\protected@file@percent }
\newlabel{fig:Transformer_architecture}{{7}{8}{Transformer-model architecture \cite {DBLP:journals/corr/VaswaniSPUJGKP17}}{figure.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Methodology}{8}{section.4}\protected@file@percent }
\newlabel{sec:methodology}{{4}{8}{Methodology}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}The Dataset}{9}{subsection.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Example of a particle decay graph. The nodes represent particles, and the edges represent the decay relationships between them.}}{9}{figure.8}\protected@file@percent }
\newlabel{fig:graph_example}{{8}{9}{Example of a particle decay graph. The nodes represent particles, and the edges represent the decay relationships between them}{figure.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Varying Layers in the DeepSet model}{10}{subsection.4.2}\protected@file@percent }
\newlabel{sec:Varying_layers_in_the_deepset_model}{{4.2}{10}{Varying Layers in the DeepSet model}{subsection.4.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Architecture of the variable DeepSet model with configurable GCN and Linear layers}}{10}{table.2}\protected@file@percent }
\newlabel{tab:deepset_gcn_variable_arc}{{2}{10}{Architecture of the variable DeepSet model with configurable GCN and Linear layers}{table.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Exploiting Rotational Symmetry}{11}{subsection.4.3}\protected@file@percent }
\newlabel{sec:Exploiting Rotational Symmetry}{{4.3}{11}{Exploiting Rotational Symmetry}{subsection.4.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Validation metrics for Cartesian vs. cylindrical coordinates}}{11}{table.3}\protected@file@percent }
\newlabel{tab:coordinates_investigation}{{3}{11}{Validation metrics for Cartesian vs. cylindrical coordinates}{table.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Activation functions}{12}{subsection.4.4}\protected@file@percent }
\newlabel{ssec:activations}{{4.4}{12}{Activation functions}{subsection.4.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Validation metrics for different activation functions}}{13}{table.4}\protected@file@percent }
\newlabel{tab:validation_metrics_activations}{{4}{13}{Validation metrics for different activation functions}{table.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Transforming the feature values}{13}{subsection.4.5}\protected@file@percent }
\newlabel{sec:Normalize features}{{4.5}{13}{Transforming the feature values}{subsection.4.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6}Unecessary Features}{13}{subsection.4.6}\protected@file@percent }
\newlabel{sec:Unecessary Features}{{4.6}{13}{Unecessary Features}{subsection.4.6}{}}
\citation{goodfellow2016dlbook}
\citation{ioffe2015batchnorm}
\citation{srivastava2014dropout}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Validation accuracy and loss for different experiments. The horizontal dashed lines indicate the best performance achieved with the baseline model. Note: the \textbf  {reversed} model refers to the configuration where the GCN and DeepSet layers were swapped.}}{14}{figure.9}\protected@file@percent }
\newlabel{fig:Feature_importance}{{9}{14}{Validation accuracy and loss for different experiments. The horizontal dashed lines indicate the best performance achieved with the baseline model. Note: the \textbf {reversed} model refers to the configuration where the GCN and DeepSet layers were swapped}{figure.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7}Overfitting and Mitigation Techniques}{14}{subsection.4.7}\protected@file@percent }
\newlabel{ssec:overfitting}{{4.7}{14}{Overfitting and Mitigation Techniques}{subsection.4.7}{}}
\citation{goodfellow2016dlbook}
\citation{akiba2019optuna}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.8}MLP and Masking}{16}{subsection.4.8}\protected@file@percent }
\newlabel{sec:method_flatmlp}{{4.8}{16}{MLP and Masking}{subsection.4.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.9}PDG Embedding Layer Inspection}{17}{subsection.4.9}\protected@file@percent }
\newlabel{sec:embedding_study}{{4.9}{17}{PDG Embedding Layer Inspection}{subsection.4.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces t‐SNE projection of the PDG embeddings. Six common species are highlighted to illustrate that particles and their antiparticles (e.g.\ \(\pi ^\pm \), \(p/\bar  p\)), here plotted with the same colour, are close to each other.}}{18}{figure.10}\protected@file@percent }
\newlabel{fig:pdg_tsne}{{10}{18}{t‐SNE projection of the PDG embeddings. Six common species are highlighted to illustrate that particles and their antiparticles (e.g.\ \(\pi ^\pm \), \(p/\bar p\)), here plotted with the same colour, are close to each other}{figure.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Histogram of off‐diagonal cosine similarities among PDG embeddings. Most pairs are near orthogonal (peak near 0), with tails extending toward \(\pm 1\) for strongly related or strongly opposed species.}}{19}{figure.11}\protected@file@percent }
\newlabel{fig:pdg_hist}{{11}{19}{Histogram of off‐diagonal cosine similarities among PDG embeddings. Most pairs are near orthogonal (peak near 0), with tails extending toward \(\pm 1\) for strongly related or strongly opposed species}{figure.11}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Summary of cosine‐similarity statistics for PDG embeddings.}}{19}{table.5}\protected@file@percent }
\newlabel{tab:pdg_embedding_stats}{{5}{19}{Summary of cosine‐similarity statistics for PDG embeddings}{table.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.10}Speedup}{19}{subsection.4.10}\protected@file@percent }
\newlabel{sec:Speedup}{{4.10}{19}{Speedup}{subsection.4.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.11}ROC Curve and AUC Evaluation}{20}{subsection.4.11}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Definitions}{20}{section*.14}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Interpretation}{21}{section*.15}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Summary of Evaluated Architectures}{21}{section.5}\protected@file@percent }
\newlabel{sec:models}{{5}{21}{Summary of Evaluated Architectures}{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}General setup}{21}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Deep Set Model}{21}{subsection.5.2}\protected@file@percent }
\newlabel{model:DeepSet}{{5.2}{21}{Deep Set Model}{subsection.5.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Deep Set Model Architecture}}{22}{table.6}\protected@file@percent }
\newlabel{tab:deepset_arc}{{6}{22}{Deep Set Model Architecture}{table.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Combined Model}{22}{subsection.5.3}\protected@file@percent }
\newlabel{model:CombinedModel}{{5.3}{22}{Combined Model}{subsection.5.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Architecture of the Combined Model with PDG embedding and Deep Set structure}}{22}{table.7}\protected@file@percent }
\newlabel{tab:deepset_combined_arc}{{7}{22}{Architecture of the Combined Model with PDG embedding and Deep Set structure}{table.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}DeepSet with GCN}{22}{subsection.5.4}\protected@file@percent }
\newlabel{model:DeepSet_wGCN}{{5.4}{22}{DeepSet with GCN}{subsection.5.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces Architecture of the Deep Set structure with additional GCN layer}}{23}{table.8}\protected@file@percent }
\newlabel{tab:deepset_wgcn_arc}{{8}{23}{Architecture of the Deep Set structure with additional GCN layer}{table.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}Combined Deep Set with GCN}{23}{subsection.5.5}\protected@file@percent }
\newlabel{model:CombinedModel_wGCN}{{5.5}{23}{Combined Deep Set with GCN}{subsection.5.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {9}{\ignorespaces Architecture of the Combined Model with PDG embedding, GCN and Deep Set structure}}{23}{table.9}\protected@file@percent }
\newlabel{tab:deepset_gcn_combined_arc}{{9}{23}{Architecture of the Combined Model with PDG embedding, GCN and Deep Set structure}{table.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6}Combined Deep Set with GCN Normalized}{23}{subsection.5.6}\protected@file@percent }
\newlabel{model:CombinedModel_wGCN_Normalized}{{5.6}{23}{Combined Deep Set with GCN Normalized}{subsection.5.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7}Transformer Model}{23}{subsection.5.7}\protected@file@percent }
\newlabel{model:Transformer}{{5.7}{23}{Transformer Model}{subsection.5.7}{}}
\@writefile{lot}{\contentsline {table}{\numberline {10}{\ignorespaces Architecture of the Transformer Model with PDG embedding}}{24}{table.10}\protected@file@percent }
\newlabel{tab:transformer_arc}{{10}{24}{Architecture of the Transformer Model with PDG embedding}{table.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.8}Performance Comparison}{24}{subsection.5.8}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {11}{\ignorespaces Train and validation metrics for every model}}{24}{table.11}\protected@file@percent }
\newlabel{tab:train_validation_metrics}{{11}{24}{Train and validation metrics for every model}{table.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Results}{25}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Optimal model}{25}{subsection.6.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {12}{\ignorespaces Architecture of the Optimal Model}}{25}{table.12}\protected@file@percent }
\newlabel{tab:optimalmodel_arc}{{12}{25}{Architecture of the Optimal Model}{table.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}ROC curves and speedup results}{25}{subsection.6.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces The resulting ROC curves of our different models}}{26}{figure.12}\protected@file@percent }
\newlabel{fig:ROC_curves_results}{{12}{26}{The resulting ROC curves of our different models}{figure.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Speedup gained by our separate models}}{27}{figure.13}\protected@file@percent }
\newlabel{fig:speedup}{{13}{27}{Speedup gained by our separate models}{figure.13}{}}
\@writefile{lot}{\contentsline {table}{\numberline {13}{\ignorespaces Evaluation metrics on the test dataset for all models}}{27}{table.13}\protected@file@percent }
\newlabel{tab:model_summary}{{13}{27}{Evaluation metrics on the test dataset for all models}{table.13}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{28}{section.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {A}Code Listings}{29}{appendix.A}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}Normalization function}{29}{subsection.A.1}\protected@file@percent }
\newlabel{app:normalization}{{A.1}{29}{Normalization function}{subsection.A.1}{}}
\bibstyle{plain}
\bibdata{mybib}
\bibcite{akiba2019optuna}{1}
\bibcite{belle2-image}{2}
\bibcite{belle2-Y-resonances}{3}
\bibcite{goodfellow2016dlbook}{4}
\bibcite{ioffe2015batchnorm}{5}
\bibcite{ai-lab-repo}{6}
\bibcite{srivastava2014dropout}{7}
\bibcite{turing}{8}
\bibcite{zhihu-figure-qc}{9}
\bibcite{DBLP:journals/corr/VaswaniSPUJGKP17}{10}
\bibcite{zaheer2017deep}{11}

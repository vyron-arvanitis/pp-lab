\section{Methodology}

To identify the optimal model architecture, each group member conducted 
small-scale investigations targeting specific aspects of the model design. 
These investigations included the following:

\begin{itemize}
    \item Does it help to increase the number of linear layers? Which sequence of GCN and Linear layers works best?
    \item Is it possible to use a standard MLP as well? If yes how?
    \item How important is the masking in this case?
    \item Methods to combat overfitting
    \item Do we need to transform the values for the particle features?
    \item Are there any redundant features in the dataset that could be removed?
    \item Could one make use of some (approximate) symmetries in the data? 
    \item Ideas for different model architectures
\end{itemize}


\subsection{The Dataset}

The dataset of the experiment consists of simulated particle collision events, with 
each event containing multiple decay particles. These events contain a variable number of final state and intermediate particles. Hence, they can be structured as a graph, where each particle is a node and the edges represent the relationships between particles (e.g., parent-child relationships in decays). The graph can be seen in figure \ref{fig:graph_example}.

\begin{figure}[H]
 \centering
    \includegraphics[width=0.75\textwidth]{chapters/methodology/particle_graph.png}
    \caption{Example of a particle decay graph. The nodes represent particles, 
    and the edges represent the decay relationships between them.}
    \label{fig:graph_example}
\end{figure}

Each particle is described by a fixed number of features, which include:

\begin{itemize}
    \item \texttt{prodTime} — the production time of the particle,
    \item \texttt{energy} — the energy of the particle,
    \item \texttt{x}, \texttt{y}, \texttt{z} — the spatial coordinates of the particle at production,
    \item \texttt{px}, \texttt{py}, \texttt{pz} — the momentum components of the particle,
    \item \texttt{pdg} — the PDG identifier of the particle, indicating its type,
    \item \texttt{index}, \texttt{mother\_index} — the particle’s index and the index of its mother particle (if any).
    \item \texttt{label} — this indicates weather or not the particle has passed the downstream event selection or not, it takes binary values 0 or 1
\end{itemize}

Each event is represented as a tensor of shape \texttt{(num\_particles, num\_features)}. 
Since the number of particles can vary between events, padding is applied to ensure consistent input dimensions across batches. These padded values are properly masked during training to prevent them from affecting the model.

\subsection{Varying Layers in the DeepSet model}

The architecture was developed to explore the optimal number and arrangement of 
GCN and linear layers. To support this flexibility, an additional script (deepset\_gcn\_variator.py) was introduced. This script allows dynamic model contruction and naming based on the layer configuration. 

The naming convention follows the pattern: 
\begin{center}
\texttt{DS\_\{TotalLayers\}\_GCN\_\{GCNIndices\}}.
\end{center}
For example, the model \texttt{DS\_6\_GCN\_135} consists of six layers (including input and output), with the 1st, 3rd, and 5th layers implemented as GCN layers. All other layers default to linear.

This models introduces two additional configuration arguments:
\begin{itemize}
    \item \texttt{hidden\_layers}  (int): Number of hidden layers. The total number of layers is \texttt{hidden\_layers} + 3 (one input and two output layers)
    \item \texttt{gcn\_layers} (list): A list of layer indices to be implemented as GCN layers. For example \texttt{gcn\_layers} = [2,4] replaces the 2nd and 4th layers with GCNs. 
\end{itemize}

Layer behaviour is defined as follows:
\begin{itemize}
\item If index 1 is included in \texttt{gcn\_layers}, the input layer is implemented as a GCN; otherwise, it is a linear layer. 
\item Hidden layers not specified in \texttt{gcn\_layers} are standard linear layers. 
\item After all hidden layers, aggregation is performed using masked mean pooling (or mean over $N$ if no mask is provided).
\item  The final output module consits of two linear layers speparated by a ReLU activation (i.e., a small MLP).
\end{itemize}

The complete architecture is summarized in table \ref{tab:deepset_gcn_variable_arc}.

\begin{table}[h]
    \caption{Architecture of the variable DeepSet model with configurable GCN and Linear layers}
    \label{tab:deepset_gcn_variable_arc}
    \centering
    \begin{tabular}{l|l|c|c}
        \hline
        Layer \# & Description & Output shape & Parameters \\
        \hline 
        1& Input layer (GCN or Linear with ReLU) & $[B,N,u]$ & $u \times f + u$ \\
        2 - (h+1)& Hidden layers (GCN or Linear with ReLU)& $[B,N,u]$&$u \times u + u$ (per layer) \\
        h+2& Masked mean pooling (or mean over $N$)& $[B,u]$& -- \\
        h+3& Linear + ReLU (global MLP) & $[B,u]$ & $u \times u + u$ \\ 
        h+4& Linear (output layer) & $[B,1]$ & $1 \times u + 1$ \\
        \hline
    \end{tabular}
\end{table}

\subsection{Exploiting Rotational Symmetry}

Ideally, the collision of the electrons occurs parallel to the walls of the detector. This setup allows one to introduce
cylindrical coordinates and exploit rotational symmetry around the $z$ axis. In this coordinate system, the angular component can be neglected, and the transformation is defined as
\begin{equation}
    r = \sqrt{x^2 + y^2}, \quad \text{and} \quad p_{xy} = \sqrt{p_x^2 + p_y^2}.
\end{equation}
Here, $r$ denotes the radial position, and $p_{xy}$ is the transverse momentum in the $xy$ plane. This transformation reduces the feature vector by
two dimensions, thereby lowering the risk of overfitting without discarding relevant information about the particle collision.

The implementation requires an additional function, \texttt{transform\_to\_cylindrical}, located in \texttt{utils.py}. This function takes
a DataFrame as input, which must contain at least the features \texttt{'x'}, \texttt{'y'}, \texttt{'px'}, and \texttt{'py'}. The output is a modified DataFrame in which the original
features are replaced by \texttt{'r'}, and \texttt{'p\_xy'}. 

To enebale coordinate transformation, the function \texttt{preprocess} in \texttt{utils.py} takes an additional argument, 
\texttt{"coordinates"}, which can be set to \texttt{"cylindrical"}. If this argument is not specified, the data is processed using the original Cartesian coordinates. 

\subsection{Transforming the feature values} \label{sec:Nomralize features}

For this investigation, we used our best model obtained after the first 
lab day — that is \textbf{CombinedModel with GCN} model — and focused on normalizing
the inputs. The implementation details can be found in Appendix~\ref{app:normalization}.

Essentially, subtracting the mean and dividing by the standard deviation of each feature, 
calculated across both the batch and particle dimensions. 

\subsection{Unecessary Features}

To investigate whether or not some features are unnecessary in our analysis,
we used again the same model (\textbf{CombinedModel with GCN}) and evaluated it,
subtracting features via physical arguments. We performed six tests. 

First of all, we removed the momentum vector ($\vec{p}$). The argument for this was that 
the energy and momenta of a particle are associated with the well-known relationship:
\[
E^2 = p^2 + m^2
\]
so, in principle, the model could reconstruct the momenta from the energy (or vice versa), assuming the mass is constant or implicitly learned.

For the same reason mentioned above, we explored removing the energy of the particles.

Afterwards, we removed the position vectors $x$, $y$, and $z$. 
This started as a simple test, however the following argument was made: 
in our problem, there exists a cylindrical symmetry — the $x$ and $y$ axes can be arbitrarily chosen (as the beams are considered to be colliding along the $z$ axis). Therefore, we considered removing only the $x$ and $y$ coordinates. 

However, after consideration we came to the conclusion that this is 
physically wrong because removing both $x$ and $y$ would eliminate any 
information about the transverse plane. Therefore, we opted to remove only the 
$x$ position coordinate, as a compromise to test sensitivity to symmetry 
arguments.

In the end, we removed the \texttt{prodTime} feature just for further 
analysis, since its physical importance was unclear and we wanted to check 
if it had any significant impact on performance.

\begin{figure}[H]
 \centering
    \includegraphics[width=0.75\textwidth]{chapters/methodology/feature_importance_comparison.png}
    \caption{Validation accuracy and loss for different experiments. 
    The horizontal dashed lines indicate the best performance 
    achieved with the baseline model. Note: the \textbf{reversed} 
    model refers to the configuration where the GCN 
    and DeepSet layers were swapped.}
    \label{fig:Feature_importance}
\end{figure}

From figure \ref{fig:Feature_importance} it is evident that no significant 
improvement in model performance was observed when removing features. In all 
cases, the validation accuracy and loss remained close to the baseline values. The only 
slight performance gain was observed when applying normalization to the input 
features, confirming its importance as a preprocessing step.
    

\subsection{Overfitting and Mitigation Techniques}

Overfitting occurs when a model learns not only the underlying patterns in the training data but also noise or statistical fluctuations that do not generalize to new, unseen data. This results in low training loss but poor performance on validation or test sets. In high-energy physics tasks, overfitting can severely undermine the model's ability to detect meaningful physical signatures, especially in the presence of detector noise and variable event topologies.

To combat overfitting, we employed several regularization strategies that improve generalization:

\subsection*{Early Stopping}
Early stopping halts training when the validation loss stops improving, even if the training loss continues to decrease~\cite{goodfellow2016dlbook}. This prevents the model from over-optimizing on the training data, offering a simple yet effective guard against overfitting.

\subsection*{Batch Normalization}
Batch normalization normalizes the activations within each mini-batch to have zero mean and unit variance~\cite{ioffe2015batchnorm}. This reduces internal covariate shift and stabilizes learning, which allows for higher learning rates and often acts as an implicit regularizer, discouraging overfitting.

\subsection*{Dropout}
Dropout randomly sets a subset of neurons to zero during training~\cite{srivastava2014dropout}, forcing the network to develop redundant representations. This reduces co-adaptation of neurons and enhances robustness, as the model cannot rely on any single pathway to make predictions.

\subsection*{Regularization (L1/L2)}
Regularization adds a penalty term to the loss function based on the magnitude of the model's weights. L2 regularization (weight decay) discourages overly large weights by penalizing their squared magnitude. This constrains model complexity and promotes simpler, more generalizable solutions~\cite{goodfellow2016dlbook}.

\subsection*{Increased Training Data}
Expanding the training set—either through real data or augmentation—exposes the model to a broader range of examples, making it harder to memorize specific samples. More diverse data reduces variance and improves the model's ability to generalize. In physics applications, techniques such as event reweighting or simulation-based augmentation can help increase effective training diversity.

Collectively, these methods reduce model variance and promote generalization, which is essential for ensuring reliable performance on physical inference tasks.

\subsection{Hyperparameter Optimization}

Hyperparameter optimization involves empirically testing different combinations of parameters that regulate the learning behavior of the neural network.
Key hyperparameters include the number of layers, neurons per layer, activation function, dropout rate, L2-regularization parameter $\lambda$, and learning
rate.
The Optuna library~\cite{akiba2019optuna} was used for this task. Intervals for the hyperparameters were defined based on typical values found in the literature. Optuna
then sampled values from these intervals and trained the network, storing
the final validation loss. After multiple trials, the combination of hyperparameters that yielded the lowest validation loss was selected.
Rather than a random search, Optuna uses a statistical method called the
Tree-structured Parzen Estimator (TPE). TPE models the relationship between hyperparameters and the objective function (validation loss) and focuses on sampling hyperparameters that are more likely to perform well. This method helps guide the search toward promising areas in the hyperparameter space, improving both efficiency and accuracy.

\subsection{Speedup}  \label{sec:Speedup}

To evaluate the efficiency of using a NN as a filter before detector simulation, we define the speedup as the ratio of time needed to process events \emph{without} and \emph{with} the NN, for the same number of positively selected (skimmed) events.

Let:
\begin{itemize}
    \item \( t_g \): time for event generation
    \item \( t_s \): time for detector simulation and reconstruction
    \item \( r = 0.05 \): fraction of events passing the original skim (without NN)
    \item \( f_1 \): true positive rate (TPR)
    \item \( f_0 \): false positive rate (FPR)
    \item \( X \): number of events generated
\end{itemize}

Further, we define:
\begin{align*}
    X_i &= r \cdot X \quad \text{(number of events passing skim)} \\
    X_i &= f_1 \cdot X' \quad \text{(true positives from NN)} \\
    \Rightarrow X' &= \frac{r}{f_1} \cdot X \quad \text{(number of events to be generated with NN)} \\
    P &= f_1 \cdot X' + f_0 \cdot (1 - r) \cdot X \quad \text{(events passing NN)} \\
\end{align*}

The speedup is the ratio of total time without and with NN:
\[
\text{Speedup} = \frac{X \cdot (t_g + t_s)}{X' \cdot t_g + P \cdot t_s}
\]

Substituting \( X' \) and \( P \) into the equation:
\[
\text{Speedup} = \frac{f_1 \cdot (t_g + t_s)}{t_g + t_s \cdot \left( f_1 + f_0 \cdot \frac{1 - r}{r} \right)}
\]

Finally, assuming \( t_s = 100 \cdot t_g \), we simplify to:
\[
\text{Speedup} = \frac{101 \cdot f_1}{1 + 100 \cdot \left( f_1 + f_0 \cdot \left( \frac{1 - r}{r} \right) \right)}
\]







\section{Methodology}

To identify the optimal model architecture, each group member conducted 
small-scale investigations targeting specific aspects of the model design. 
These investigations included the following:

\begin{itemize}
    \item Does it help to increase the number of linear layes?
    \item Which sequence of GCN and Linear layers works best?
    \item Is it possible to use a standard MLP as well? If yes how?
    \item How important is the masking in this case?
    \item Methods to combat overfitting
    \item Do we need to transform the values for the particle features?
    \item Are there any redundant features in the dataset that could be removed?
    \item Could one make use of some (approximate) symmetries in the data? 
    \item Ideas for different model architectures
\end{itemize}


\subsection{The Dataset}

The dataset of the experiment consists of simulated particle collision events, with 
each event containing multiple decay particles. These events contain a variable number of final state and intermediate particles. Hence, they can be structured as a graph, where each particle is a node and the edges represent the relationships between particles (e.g., parent-child relationships in decays). The graph can be seen in figure \ref{fig:graph_example}.

\begin{figure}[H]
 \centering
    \includegraphics[width=0.75\textwidth]{chapters/methodology/particle_graph.png}
    \caption{Example of a particle decay graph. The nodes represent particles, 
    and the edges represent the decay relationships between them.}
    \label{fig:graph_example}
\end{figure}

Each particle is described by a fixed number of features, which include:

\begin{itemize}
    \item \texttt{prodTime} — the production time of the particle,
    \item \texttt{energy} — the energy of the particle,
    \item \texttt{x}, \texttt{y}, \texttt{z} — the spatial coordinates of the particle at production,
    \item \texttt{px}, \texttt{py}, \texttt{pz} — the momentum components of the particle,
    \item \texttt{pdg} — the PDG identifier of the particle, indicating its type,
    \item \texttt{index}, \texttt{mother\_index} — the particle’s index and the index of its mother particle (if any).
    \item \texttt{label} — this indicates weather or not the particle has passed the downstream event selection or not, it takes binary values 0 or 1
\end{itemize}

Each event is represented as a tensor of shape \texttt{(num\_particles, num\_features)}. 
Since the number of particles can vary between events, padding is applied to ensure consistent input dimensions across batches. These padded values are properly masked during training to prevent them from affecting the model.

\subsection{Transforming the feature values} \label{sec:Nomralize features}

For this investigation, we used our best model obtained after the first 
lab day — that is \textbf{CombinedModel with GCN} model — and focused on normalizing
the inputs. The normalization was done according to the following function


\begin{lstlisting}
def normalize_inputs(inputs):
    x = inputs["feat"] # with "feat" we mean the position, momentum vectors, the production time and the Energy
    # x.shape = (batch_size, num_particles, num_features)
    mean = x.mean(dim=(0,1), keepdim=True)  # Collapse batch and particle dimensions; end up with (num_features) -> one mean per feature!
    std = x.std(dim=(0,1), keepdim=True) + 1e-8  # avoid divide-by-zero
    x_norm = (x - mean) / std
    return {**inputs, "feat": x_norm}
\end{lstlisting}

Essentially, we subtracted the mean and divided by the standard deviation of each feature, calculated across 
both the batch and particle dimensions. 

\subsection{Unecessary Features}

To investigate whether or not some features are unnecessary in our analysis,
we used again the same model (\textbf{CombinedModel with GCN}) and evaluated it,
subtracting features via physical arguments. We performed six tests. 

First of all, we removed the momentum vector ($\vec{p}$). The argument for this was that 
the energy and momenta of a particle are associated with the well-known relationship:
\[
E^2 = p^2 + m^2
\]
so, in principle, the model could reconstruct the momenta from the energy (or vice versa), assuming the mass is constant or implicitly learned.

For the same reason mentioned above, we explored removing the energy of the particles.

Afterwards, we removed the position vectors $x$, $y$, and $z$. 
This started as a simple test, however the following argument was made: 
in our problem, there exists a cylindrical symmetry — the $x$ and $y$ axes can be arbitrarily chosen (as the beams are considered to be colliding along the $z$ axis). Therefore, we considered removing only the $x$ and $y$ coordinates. 

However, after consideration we came to the conclusion that this is 
physically wrong because removing both $x$ and $y$ would eliminate any 
information about the transverse plane. Therefore, we opted to remove only the 
$x$ position coordinate, as a compromise to test sensitivity to symmetry 
arguments.

In the end, we removed the \texttt{prodTime} feature just for further 
analysis, since its physical importance was unclear and we wanted to check 
if it had any significant impact on performance.

\begin{figure}[H]
 \centering
    \includegraphics[width=0.75\textwidth]{chapters/methodology/feature_importance_comparison.png}
    \caption{Validation accuracy and loss for different experiments. 
    The horizontal dashed lines indicate the best performance 
    achieved with the baseline model. Note: the \textbf{reversed} 
    model refers to the configuration where the GCN 
    and DeepSet layers were swapped.}
    \label{fig:Feature_importance}
\end{figure}

From figure \ref{fig:Feature_importance} it is evident that no significant 
improvement in model performance was observed when removing features. In all 
cases, the validation accuracy and loss remained close to the baseline values. The only 
slight performance gain was observed when applying normalization to the input 
features, confirming its importance as a preprocessing step.
    

\subsection{Speedup}

To evaluate the efficiency of using a NN as a filter before detector simulation, we define the speedup as the ratio of time needed to process events \emph{without} and \emph{with} the NN, for the same number of positively selected (skimmed) events.

Let:
\begin{itemize}
    \item \( t_g \): time for event generation
    \item \( t_s \): time for detector simulation and reconstruction
    \item \( r = 0.05 \): fraction of events passing the original skim (without NN)
    \item \( f_1 \): true positive rate (TPR)
    \item \( f_0 \): false positive rate (FPR)
    \item \( X \): number of events generated
\end{itemize}

Further, we define:
\begin{align*}
    X_i &= r \cdot X \quad \text{(number of events passing skim)} \\
    X_i &= f_1 \cdot X' \quad \text{(true positives from NN)} \\
    \Rightarrow X' &= \frac{r}{f_1} \cdot X \quad \text{(number of events to be generated with NN)} \\
    P &= f_1 \cdot X' + f_0 \cdot (1 - r) \cdot X \quad \text{(events passing NN)} \\
\end{align*}

The speedup is the ratio of total time without and with NN:
\[
\text{Speedup} = \frac{X \cdot (t_g + t_s)}{X' \cdot t_g + P \cdot t_s}
\]

Substituting \( X' \) and \( P \) into the equation:
\[
\text{Speedup} = \frac{f_1 \cdot (t_g + t_s)}{t_g + t_s \cdot \left( f_1 + f_0 \cdot \frac{1 - r}{r} \right)}
\]

Finally, assuming \( t_s = 100 \cdot t_g \), we simplify to:
\[
\text{Speedup} = \frac{101 \cdot f_1}{1 + 100 \cdot \left( f_1 + f_0 \cdot \left( \frac{1 - r}{r} \right) \right)}
\]

\subsection{Models architecture}

From our comulative research we came to the conclusion that the following
are the best models, we mention also more simplistic models as it is 
interesting to see the impact that additional layers or methods of 
regularization have on the performance.

\subsubsection*{DeepSet Model} \label{model:DeepSet}

This model follows the Deep Sets framework (as mentioned in \ref{sec:DeepSet}), 
where the same transformation is applied independently to each element of the input set before aggregation.

\paragraph{Architecture.} 
The model takes as input a batch of events, where each event consists of a 
list of particles. Each particle is represented by a feature vector.
The architecture consists of the following components:

\begin{itemize}
    \item Linear layer with Relu activation function
    \item A masking layer with averaging over the masked events or averaging if no masks exists
    \item Linear layer with Relu activation function
    \item Output layer (a single neuron)

\end{itemize}

\subsubsection*{Combined Model} \label{model:CombinedModel}

This model extends the deepset architecture by introducing PDG code to each particles
afterwards mapped to an embedding vector. The embedding vectors are added to the original 
particle feature before being processed by the rest of the network

\paragraph{Architecture.}
The model processes a batch of events, where each particle is represented by both a feature vector and a PDG code. 
The architecture consists of the following components:

\begin{itemize}
    \item Embedding layer that maps each PDG code to a vector.
    \item Concatenation of the PDG embedding with the original particle features.
    \item Linear layer with ReLU activation function.
    \item A masking layer with averaging over the masked events or simple averaging if no mask is used.
    \item Linear layer with ReLU activation function.
    \item Output layer (a single neuron).
\end{itemize}

\subsubsection*{DeepSet with GCN} \label{model:DeepSet_wGCN}

This model combines a Graph Convolutional Network (GCN) layer with the Deep Set architecture.
The GCN captures local structure between particles as defined by an adjacency matrix (typically based on mother-daughter relations), 

\paragraph{Architecture.}
The input consists of a set of particles (each with a feature vector) and a corresponding adjacency matrix encoding  connections between particles. 
The architecture includes the following components:

\begin{itemize}
    \item A GCN layer that applies a linear transformation to each particle this is done by multiplying the normalized adjacency matrix with a linear layer
    \item A DeepSet layer that aggregates the GCN output using a masked mean over particles, followed by a linear transformation with ReLU activation.
    \item An output layer (a single neuron) that produces the final prediction.
\end{itemize}

\subsubsection*{Combined Deepset with GCN} \label{model:CombinedModel_wGCN}
This model extends the DeepSet with GCN architecture by introducing a  PDG code to each 
particles afterwards mapped to an embedding vector

\paragraph{Architecture.}
The input consists of particle features, PDG codes, and an adjacency matrix. 
The architecture includes the following components:

\begin{itemize}
    \item An embedding layer that maps each PDG code to a vector.
    \item A droupout layer with a droupout rate of $0.3$.
    \item Concatenation of the PDG embedding with the original particle features and the normalized adjacency matrix.
    \item A GCN layer applied to the concatenated inputs.
    \item A batch normalization layer 
    \item A dropout layer with a droupout rate of $0.3$
    \item A Deep Set layer.
    \item An output layer (a single neuron) that produces the final prediction.
\end{itemize}


\subsubsection*{Combined Deepset with GCN Normalized} \label{model:CombinedModel_wGCN_Normalized}
This model extends the Combined Deepset with GCN architecture (model \ref{model:CombinedModel_wGCN})
by introducing a normalization of the inputs according to section \ref{sec:Nomralize features}.

\paragraph{Architecture.}
The architecture is the same as in the case of \ref{model:CombinedModel_wGCN}
with an additional normalization layer.



\subsubsection*{Transformer} \label{model:Transformer}



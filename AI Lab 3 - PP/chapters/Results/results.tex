\section{Results}

\subsection{Optimal model}

Section \ref{sec:methodology} explores various strategies for regularization, feature processing, and architectural design. Based on the insights gained, we combined the most effective components into a single architecture, referred to as the Optimal Model. 

The model builds upon a base architecture in which the input features are augmented with PDG ID embeddings, increasing the feature dimensionality by 25. The network consists of four sequential layers: the input layer, second, and third layer are implemented as GCNs, followed by one linear layer. This is followed by mean pooling across particles and a final two-layer MLP to produce the output. 

Each trainable layer is followed by dropout (with a rate of $0.179$), a LeakyReLU activation (slope $0.01$), and Batch Normalization. Training was conducted using cylindrical coordinates, while all other settings remained consistent with previous experiments. 

\begin{table}[H]
    \caption{Architecture of the Optimal Model}
    \label{tab:transformer_arc}
    \centering
    \begin{tabular}{l|l|c|c}
        \hline
        Layer \# & Description & Output shape & Parameters \\
        \hline 
        1 & Embedding lookup (PDG code) & $[B,N,e]$ & $(n_\text{PDG} + 1) \times e $ \\
        -- & Concatenation of features and embeddings & $[B,N,f+e]$ & -- \\
        2 & GCN layer + BatchNorm, LeakyReLU, Dropout & $[B,N,u]$ & $u \times (f+e) + u$ \\
        3-4 & GCN layer + BatchNorm, LeakyReLU, Dropout & $[B,N,u]$ & $u \times u + u$ \\
        5 & Linear layer + BatchNorm, LeakyReLU, Dropout& $[B,N,u]$ & $u \times u + u$ \\
        --& Masked mean pooling (or mean over $N$)& $[B,u]$& -- \\
        6& Linear + BatchNorm, LeakyReLU, Dropout & $[B,u]$ & $u \times u + u$ \\ 
        7& Linear (output layer) & $[B,1]$ & $1 \times u + 1$ \\
        \hline
    \end{tabular}
\end{table}


\subsection{ROC curves and speedup results}

Figures~\ref{fig:ROC_curves_results} and~\ref{fig:speedup} present the ROC curves and speedup plots for our most promising models.  
We began with a simple Deep Set model (\ref{sec:DeepSet}) and, 
through a series of investigations, developed more complex architectures such as the Deep Set Combined model and the Deep Set Combined with GCN model.  
Our exploration culminated in a Transformer model and an ``Optimal Model'' configuration.


\begin{figure}[H]
 \centering
    \includegraphics[width=0.75\textwidth]{chapters/Results/ROC_curves.pdf}
    \caption{The resulting ROC curves of our different models}
    \label{fig:ROC_curves_results}
\end{figure}

From Figure~\ref{fig:ROC_curves_results}, it is evident that all but 
the simplest baseline models achieve strong results,  
with most AUC scores exceeding $0.85$, indicating strong classification performance.  
The Transformer and Optimal Model stand out with the highest AUC values.

While ROC curves indicate classification quality, they do not reflect entirely the impact
on computational efficiency. To address this, we also evaluate the \emph{speedup}—(\ref{sec:Speedup})
if events are filtered using the model predictions.

\begin{figure}[H]
 \centering
    \includegraphics[width=0.75\textwidth]{chapters/Results/Speedup.png}
    \caption{Speedup gained by our seperate models}
    \label{fig:speedup}
\end{figure}

From figure  (fig \ref{fig:ROC_curves_results}), we observe that the
more advanced models are indistinguishable as far as their ROC performance is concerned,
however they diverge in the achievable speedup. 
The Optimal Model attains the highest maximum speedup, 
followed closely by the Transformer and Deep Set Combined with GCN variants.


\begin{table}[h]
    \centering
    \caption{Evaluation metrics for all models}
    \label{tab:model_summary}
    \begin{tabular}{l|c|c|c|c|c}
        \hline
        \textbf{Model} & \textbf{Loss} & \textbf{Acc. (\%)} & \textbf{AUC} & \textbf{Max. Speedup} & \textbf{Best Threshold} \\
        \hline
        DS & $0.633$ & $63.6$ & $0.699$ & $2.485$ & $0.618$ \\
        DS\_combined & $0.476$ & $77.5$ & $0.852$ & $4.758$ & $1.757$ \\
        DS\_gcn & $0.614$ & $66.0$ & $0.721$ & $2.795$ & $1.149$ \\
        DS\_combined\_wgcn & $0.464$ & $78.0$ & $0.862$ & $5.323$ & $1.586$ \\
        transformer & $0.441$ & $79.5$ & $0.872$ & $5.301$ & $1.774$ \\
        DS\_combined\_wgcn\_normalized & $0.467$ & $77.6$ & $0.860$ & $5.373 & $1.71988$ \\
        optimal\_model & $0.432$ & $80.1$ & $0.880$ & $6.009$ & $1.899$ \\
        \hline
    \end{tabular}
\end{table}

The \emph{Best Threshold} column is obtained from the set 
of decision thresholds returned by the \verb|roc_curve| function.  
These thresholds correspond to decreasing cut values on 
the model’s decision function that are used to compute 
the true positive rate (TPR) and false positive rate (FPR) pairs.  
The first threshold is always set to \verb|np.inf|, 
which by definition yields zero predicted positives.  

Among these thresholds, the one shown in the table is 
the value at which the model 
achieves its maximum estimated speedup.  
For example, the Optimal Model reaches a 
maximum speedup of $5.934$ at a decision threshold of $1.901$.

\section{Models Used in the End}
Based on our comulative research, we identified the following models as the most promising. We also include simpler baseline architectures, as they help illustrate the impact of additional layers and regularization techniques on model performace. 

\subsection{General setup}
All models described below take at least the following arguments:
\begin{itemize}
    \item \texttt{num\_features} (int): Number of input features
    \item \texttt{units} (int): Number of hidden units
\end{itemize}

To describe the shapes of the model layers, we adopt the following notation:
\begin{itemize}
    \item $B$: Batch size
    \item $N$: Number of particles per event
    \item $f$: Number of input features
    \item $u$: Number of hidden units
\end{itemize}

The input to every model is given by $B$ batches, each containing $N$ particles, each having $f$ features. The input shape is therefore $[B,N,f]$.
Any deviation will be further specified for each model individually.

\subsection{DeepSet Model} \label{model:DeepSet}

This model follows the Deep Sets framework (as introduced in Section \ref{sec:DeepSet}), 
in which the same transformation is applied independently to each element of the input set, followed by a permutation-invariant aggregation. All other models are built upon this base architecture. 

The architecture of the model is summarized in table \ref{tab:deepset_arc}.
\begin{table}[h]
    \caption{DeepSet Model Architecture}
    \label{tab:deepset_arc}
    \centering
    \begin{tabular}{l|l|c|c}
        \hline
        Layer \# & Description & Output shape & Parameters \\
        \hline 
        1 & Linear + ReLU (shared across particles) & $[B,N,u]$ & $u \times f + u$ \\
        2 & Masked mean pooling (or mean over $N$) & $[B,u]$ & --\\
        3 & Linear + ReLU (global MLP)& $[B,u]$ & $u \times u + u$ \\
        4 & Linear (Output layer) & $[B,1]$ & $1 \times u + 1$ \\
        \hline
    \end{tabular}
\end{table}

\subsection{Combined Model} \label{model:CombinedModel}

This model extends the DeepSet architecture by introducing an eight dimensional PDG code for each particle,
which is then mapped to an embedding vector. The resulting embeddings are concatenated with the original particle features before being processed by the rest of the network

The input to this model includes both the feature vector of each particle and the corresponding PDG ID vector.

The additional arguments introduced by this model are:
\begin{itemize}
    \item \texttt{embed\_dim:} Embedding dimension ($= e$)
    \item \texttt{num\_pdg\_ids:} Number of PDG ID categories ($= n_\text{PDG}$)
\end{itemize}

The architecture of the model is summarized in table \ref{tab:deepset_combined_arc}.
\begin{table}[h]
    \caption{Architecture of the Combined Model with PDG embedding and DeepSet structure}
    \label{tab:deepset_combined_arc}
    \centering
    \begin{tabular}{l|l|c|c}
        \hline
        Layer \# & Description & Output shape & Parameters \\
        \hline 
        1& Embedding lookup (PDG code) & $[B,N,e]$ & $(n_\text{PDG} + 1) \times e $ \\
        2& Concatenation of features and embeddings & $[B,N,f+e]$& --\\
        3& Linear + ReLU (shared across particles) & $[B,N,u]$ & $u \times (f+e) + u$ \\
        4& Masked mean pooling (or mean over $N$) & $[B,u]$ & --\\
        5& Linear + ReLU (global MLP) & $[B,u]$ & $u \times u + u$ \\
        6& Linear (output layer) & $[B,1]$ & $1 \times u + 1$ \\
        \hline
    \end{tabular}
\end{table}


\subsection{DeepSet with GCN} \label{model:DeepSet_wGCN}

This model combines a Graph Convolutional Network (GCN) layer with the Deep Set architecture.
The GCN captures local structure between particles as defined by an adjacency matrix (typically based on mother-daughter relations).

The additional input this model takes is an adjacency matrix that encodes the particles pairwise relationships. 
The adjacency matrix is a symmetric binary matrix that encodes
\begin{itemize}
\item mother-daughter and daughter-mother relationships between particles,
\item as well as self-connections (i.e., diagonal entries are 1).
\end{itemize}

This structure ensures that information can flow both directions alogn the graph edges, while also preserving each particle's self-information during message passing in the GCN. 
The models architecture is summarized in \ref{tab:deepset_wgcn_arc}

\begin{table}[h]
    \caption{Architecture of the Combined Model with PDG embedding and DeepSet structure}
    \label{tab:deepset_wgcn_arc}
    \centering
    \begin{tabular}{l|l|c|c}
        \hline
        Layer \# & Description & Output shape & Parameters \\
        \hline 
        1& Graph Convolution (GCN) & $[B,N,u]$ & $u \times f + u$ \\
        2& Linear + ReLU (shared across particles) & $[B,N,u]$ & $u \times u + u$ \\
        3& Masked mean pooling (or mean over $N$)& $[B,u]$& -- \\
        4& Linear + ReLU (global MLP) & $[B,u]$ & $u \times u + u$ \\ 
        5& Linear (output layer) & $[B,1]$ & $1 \times u + 1$ \\ 
        \hline
    \end{tabular}
\end{table}

\subsection{Combined Deepset with GCN} \label{model:CombinedModel_wGCN}
This model combines the architectures of the "DeepSet with GCN" (section \ref{model:DeepSet_wGCN}) and the "Combined Model" (section \ref{model:CombinedModel}): 
The PDG ID of each particle is mapped to an embedding vector and concatenated with its feature vector. The resulting enriched representation is then processes by the DeepSet-GCN architecture.

Additionally the input requires the PDG codes, and the adjacency matrix as meantioned in section \ref{model:DeepSet_wGCN}. 
The architecture of the model is summarized in table \ref{tab:deepset_gcn_combined_arc}

\begin{table}[h]
    \caption{Architecture of the Combined Model with PDG embedding, GCN and DeepSet structure}
    \label{tab:deepset_gcn_combined_arc}
    \centering
    \begin{tabular}{l|l|c|c}
        \hline
        Layer \# & Description & Output shape & Parameters \\
        \hline 
        1& Embedding lookup (PDG code) & $[B,N,e]$ & $(n_\text{PDG} + 1) \times e $ \\
        2& Concatenation of features and embeddings & $[B,N,f+e]$& --\\
        3& Graph Convolution (GCN) & $[B,N,u]$ & $u \times (e+f) + u$ \\
        4& Linear + ReLU (shared across particles) & $[B,N,u]$ & $u \times u + u$ \\
        5& Masked mean pooling (or mean over $N$)& $[B,u]$& -- \\
        6& Linear + ReLU (global MLP) & $[B,u]$ & $u \times u + u$ \\ 
        7& Linear (output layer) & $[B,1]$ & $1 \times u + 1$ \\
        \hline
    \end{tabular}
\end{table}

\subsection{Combined Deepset with GCN Normalized} \label{model:CombinedModel_wGCN_Normalized}
This model extends the Combined Deepset with GCN architecture (model \ref{model:CombinedModel_wGCN})
by introducing a normalization of the inputs according to section \ref{sec:Nomralize features}.

\paragraph{Architecture}
The architecture is the same as in the case of \ref{model:CombinedModel_wGCN}
with an additional normalization layer.

\subsection{Transformer Model} \label{model:Transformer}
This architecture is quite different from the Graph Convolution 
and Deep Set approaches explored earlier.  
The Transformer leverages self-attention, a mechanism that enables the model to learn long-range 
interactions between particles — a capability particularly valuable in 
particle physics tasks.


The input to the Transformer model consists of both the particle feature vectors and their corresponding PDG codes.  
The PDG codes are first mapped to embedding vectors, which are concatenated with the original particle features.  
The combined representation is then projected to the Transformer’s internal dimension before being passed  
through a stack of Transformer encoder layers.

The additional arguments introduced by this model are:
\begin{itemize}
    \item \texttt{embed\_dim} (int): Embedding dimension ($= e$)
    \item \texttt{num\_pdg\_ids} (int): Number of PDG ID categories ($= n_\text{PDG}$)
    \item \texttt{num\_heads} (int): Number of attention heads in each Transformer layer
    \item \texttt{num\_layers} (int): Number of Transformer encoder layers
    \item \texttt{dropout\_rate} (float): Dropout probability applied in the Transformer layers
\end{itemize}

The architecture of the Transformer model is summarized in Table~\ref{tab:transformer_arc}.

\begin{table}[h]
    \caption{Architecture of the Transformer Model with PDG embedding}
    \label{tab:transformer_arc}
    \centering
    \begin{tabular}{l|l|c|c}
        \hline
        Layer \# & Description & Output shape & Parameters \\
        \hline 
        1 & Embedding lookup (PDG code) & $[B,N,e]$ & $(n_\text{PDG} + 1) \times e $ \\
        2 & Concatenation of features and embeddings & $[B,N,f+e]$ & -- \\
        3 & Linear projection to Transformer units & $[B,N,u]$ & $u \times (f+e) + u$ \\
        4 & Transformer encoder stack & $[B,N,u]$ & per-layer: MHSA + FFN parameters \\
        5 & Mean pooling over particle dimension & $[B,u]$ & -- \\
        6 & Linear output layer & $[B,1]$ & $1 \times u + 1$ \\
        \hline
    \end{tabular}
\end{table}


\subsection{Optimal Model}
\textbf{This should probably be moved on the chapter above}
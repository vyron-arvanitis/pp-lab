\section{Models}
Based on our comulative research, we dentified the following models as the most promising. We also include simpler baseline architectures, as they help illustrate the impact of additional layers and regularization techniques on model performace. 

\subsection{General setup}
All models described below take at least the following arguments:
\begin{itemize}
    \item \texttt{num\_features}: Number of input features
    \item \texttt{units}: Number of hidden units
\end{itemize}

To describe the shapes of the model layers, we adopt the following notation:
\begin{itemize}
    \item $B$: Batch size
    \item $N$: Number of particles per event
    \item $f$: Number of input features
    \item $u$: Number of hidden units
\end{itemize}

\subsection{DeepSet Model} \label{model:DeepSet}

This model follows the Deep Sets framework (as introduced in Section \ref{sec:DeepSet}), 
in which the same transformation is applied independently to each element of the input set, followed by a permutation-invariant aggregation. All other models are built upon this base architecture. 

This model solely uses the feature vector of each particle and the specified arguments as input for its configuration.

The architecture of the model is given by table \ref{tab:deepset_arc}.
\begin{table}[h]
    \caption{DeepSet Model Architecture}
    \label{tab:deepset_arc}
    \centering
    \begin{tabular}{l|l|c|c}
        \hline
        Layer \# & Description & Output shape & Parameters \\
        \hline 
        1 & Linear + ReLU (shared across particles) & $[B,N,u]$ & $u \times f + u$ \\
        2 & Masked mean pooling (or mean over $N$) & $[B,u]$ & --\\
        3 & Linear + ReLU (global MLP)& $[B,u]$ & $u \times u + u$ \\
        4 & Linear (Output layer) & $[B,1]$ & $1 \times u + 1$ \\
        \hline
    \end{tabular}
\end{table}

\subsection{Combined Model} \label{model:CombinedModel}

This model extends the DeepSet architecture by introducing a PDG code for each particle,
which is then mapped to an embedding vector. The resulting embeddings are concatenated with the original particle features before being processed by the rest of the network

The input to this model includes both the feature vector of each particle and the corresponding PDG ID vector.

The additional arguments introduced by this model are:
\begin{itemize}
    \item \texttt{embed\_dim:} Embedding dimension ($= e$)
    \item \texttt{num\_pdg\_ids:} Number of PDG ID categories ($= n_\text{PDG}$)
\end{itemize}

The architecture of the model is summarized in table \ref{tab:deepset_combined_arc}.
\begin{table}[h]
    \caption{Architecture of the Combined Model with PDG embedding and DeepSet structure}
    \label{tab:deepset_combined_arc}
    \centering
    \begin{tabular}{l|l|c|c}
        \hline
        Layer \# & Description & Output shape & Parameters \\
        \hline 
        1& Embedding lookup (PDG code) & $[B,N,e]$ & $(n_\text{PDG} + 1) \times e $ \\
        2& Concatenation of features and embeddings & $[B,N,f+e]$& --\\
        3& Linear + ReLU (shared across particles) & $[B,N,u]$ & $u \times (f+e) + u$ \\
        4& Masked mean pooling (or mean over $N$) & $[B,u]$ & --\\
        5& Linear + ReLU (global MLP) & $[B,u]$ & $u \times u + u$ \\
        6& Linear (output layer) & $[B,1]$ & $1 \times u + 1$ \\
        \hline
    \end{tabular}
\end{table}


\subsection{DeepSet with GCN} \label{model:DeepSet_wGCN}

This model combines a Graph Convolutional Network (GCN) layer with the Deep Set architecture.
The GCN captures local structure between particles as defined by an adjacency matrix (typically based on mother-daughter relations).

The input consists of a set of particles (each with a feature vector) and a corresponding adjacency matrix encoding connections between particles. 

This model takes as input the feature vectors of all particles along with an adjacency matrix that encodes their pairwise relationships. 
The adjacency matrix used a input to the model is constructed form mother-daughter relations. It is a symmetric binary matrix that encodes
\begin{itemize}
\item mother-daugther and daughter-mother rleationships between particles,
\item as well as self-connections (i.e., diagonal entries are 1).
\end{itemize}

This structure ensures that information can flow both directions alogn the graph edges, while also preserving each particle's self-information during message passing in the GCN. 
The models architecture is summarized in \ref{tab:deepset_wgcn_arc}

\begin{table}[h]
    \caption{Architecture of the Combined Model with PDG embedding and DeepSet structure}
    \label{tab:deepset_wgcn_arc}
    \centering
    \begin{tabular}{l|l|c|c}
        \hline
        Layer \# & Description & Output shape & Parameters \\
        \hline 
        1& Graph Convolution (GCN) & $[B,N,u]$ & $u \times f + u$ \\
        2& Linear + ReLU (shared across particles) & $[B,N,u]$ & $u \times u + u$ \\
        3& Masked mean pooling (or mean over $N$)& $[B,u]$& -- \\
        4& Linear + ReLU (global MLP) & $[B,u]$ & $u \times u + u$ \\ 
        5& Linear (output layer) & $[B,1]$ & $1 \times u + 1$ \\ & \\
        \hline
    \end{tabular}
\end{table}

\subsection{Combined Deepset with GCN} \label{model:CombinedModel_wGCN}
This model extends the DeepSet with GCN architecture by introducing a PDG code to each 
particles afterwards mapped to an embedding vector.

The input consists of particle features, PDG codes, and an adjacency matrix. 
The architecture of the model is summarized in table \ref{tab:deepset_gcn_combined_arc}

\begin{table}[h]
    \caption{Architecture of the Combined Model with PDG embedding, GCN and DeepSet structure}
    \label{tab:deepset_gcn_combined_arc}
    \centering
    \begin{tabular}{l|l|c|c}
        \hline
        Layer \# & Description & Output shape & Parameters \\
        \hline 
        1& Embedding lookup (PDG code) & $[B,N,e]$ & $(n_\text{PDG} + 1) \times e $ \\
        2& Concatenation of features and embeddings & $[B,N,f+e]$& --\\
        3& Graph Convolution (GCN) & $[B,N,u]$ & $u \times (e+f) + u$ \\
        4& Linear + ReLU (shared across particles) & $[B,N,u]$ & $u \times u + u$ \\
        5& Masked mean pooling (or mean over $N$)& $[B,u]$& -- \\
        4& Linear + ReLU (global MLP) & $[B,u]$ & $u \times u + u$ \\ 
        5& Linear (output layer) & $[B,1]$ & $1 \times u + 1$ \\ & \\
        \hline
    \end{tabular}
\end{table}

\subsection{Combined Deepset with GCN Normalized} \label{model:CombinedModel_wGCN_Normalized}
This model extends the Combined Deepset with GCN architecture (model \ref{model:CombinedModel_wGCN})
by introducing a normalization of the inputs according to section \ref{sec:Nomralize features}.

\paragraph{Architecture}
The architecture is the same as in the case of \ref{model:CombinedModel_wGCN}
with an additional normalization layer.

\subsection{DeepSet with GCN variable}

The architecture was designed to investigate the optimal number and arrangement of 
GCN and linear layers. To support this, an additional script (deepset\_gcn\_variator.py) was introduced, enabling dynamic naming based on layer configuration. The naming convention follows the pattern: \texttt{DS\_\{TotalLayers\}\_GCN\_\{GCNIndices\}}.
For example, the model "\texttt{DS\_6\_GCN\_135}" consists of six layers in total, which are by default linear, with the 1st, 3rd, and 5th layer replaced by GCNs.

The model takes four arguments: 
\begin{itemize}
    \item \textbf{hidden\_layers (int)}: Gives the number of hidden layers. The total number of layers is then determined by the number of hidden layers + 2 (in- and output). 
    \item \textbf{gcn\_layers (list)}: A list of layer indices to be implemented as GCN layers. For example \texttt{gcn\_layers = [2,4]} replaces the 2nd and 4th layers with GCNs. 
    \item \textbf{num\_features (int)}: Defines the number of input features and determines the shape of the input layer.
    \item \textbf{units}: Sets the dimensionality of hidden layers.
\end{itemize}

If \texttt{gcn\_layers} includes the index $1$, the input layer is implemented as a GCN, otherwise a linear input layer is used by default. 
In both cases, the input layer has shape $\texttt{num\_features} \times \texttt{units}$. 

Hidden layers not specified in \texttt{gcn\_layers} are implemented as linear layers. All hidden layers, whether linear or GCN, have shape $\texttt{units} \times \texttt{units}$. The final output layer is always a linear pooling layer of shape $\texttt{units} \times 1$.

\subsection{Transformer} \label{model:Transformer}
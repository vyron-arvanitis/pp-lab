\section{Theory}
\subsection{Deep Sets}
In many physical systems, such as collections of particles or events in high-energy physics, the data can be naturally represented as unordered sets. Traditional neural networks, however, are not inherently permutation-invariant. This means that they assume an order in their input. To address this, Deep Sets provide a principled way to process such set-structured data while maintaining invariance to the order of elements. Figure \ref{fig:deepset} visualizes the structure of a Deep Set model. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{chapters/theory/deep_set.png}
    \caption{Schematic illustration of a Deep Set model.}
    \label{fig:deepset}
\end{figure}

A Deep Set model operates by applying a shared function $\phi$ independently to each element in the set. This produces a new set of embeddings, which are then aggregated using a permutation-invariant operation such as sum, mean, or max. The aggregated result is passed through a second function $\rho$, typically implemented as a multilayer perceptron (MLP), to produce the final output:

\begin{equation}
    f(X) = \rho\left( \sum_{x \in X} \phi(x) \right)
\end{equation}

This structure ensures that the model respects the symmetry of sets. The theoretical foundation of Deep Sets is discussed in Zaheer et al., 2017, where it is shown that any function on a set that is invariant to permutations can be decomposed in this form.

In our project, we use this architecture to model collections of particles, where the order of particles is physically irrelevant. The Deep Set model thus provides a natural and efficient baseline for learning from such data.

\subsection{Graph Neural Networks}
In contrast to Deep Set models, which treat the input as an unordered collection, Graph Neural Networks (GNNs) explicitly leverage relational information between data points. This is particularly important in our context, where particles are connected through a decay tree structure that naturally forms a graph. Each node in the graph represents a particle, and edges represent physical relationships (e.g., parent-child connections in the decay chain).

In terms of Deep Sets, graph convolutions can be seen as permutation-equivariant operations. As long as the aggregation function (e.g., sum, mean, or max) is symmetric, the update remains invariant to the ordering of neighbors.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\linewidth]{chapters/theory/cnn_vs_gcn.jpg}
	\caption{Analogy between CNNs and GCNs. In CNNs, pixels are updated based on spatial neighborhoods; in GCNs, nodes are updated based on graph neighborhoods.}
	\label{fig:gcn}
\end{figure}

Figure \ref{fig:gcn} illustrates the analogy between Convolutional Neural Networks (CNNs) and Graph Convolutional Networks (GCNs). In CNNs, pixels are updated based on their spatial neighborhoods, while in GCNs, nodes are updated based on their graph neighborhoods. This allows GNNs to capture complex relationships and dependencies in the data, making them particularly suitable for tasks involving structured data like particle decays.

% TODO add references:
% See [arXiv:1703.06114](https://arxiv.org/abs/1703.06114) for a detailed discussion.
% (figure from https://zhuanlan.zhihu.com/p/51990489)

\subsection{Multi Layer Perceptron}
The Multi Layer architecture is as 


\subsection{Transformer}

The transformer architecture is as 

